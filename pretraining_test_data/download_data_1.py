"""
Ultra-fast dataset download script - directly take the first N samples with no sampling overhead
"""
import os
import json
import pickle
from pathlib import Path
from datasets import load_dataset
from tqdm import tqdm
import argparse
from typing import Dict, List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import hashlib

class FastDataDownloader:
    def __init__(self,
                 output_dir: str = "data/raw",
                 cache_dir: str = ".download_cache",
                 max_workers: int = 4,
                 use_mirror: bool = True):
        """
        Initialize the fast downloader

        Parameters:
            output_dir: Output directory
            cache_dir: Cache directory (for resume/checkpointing)
            max_workers: Maximum number of parallel download threads
            use_mirror: Whether to use a domestic mirror site
        """
        self.output_dir = Path(output_dir)
        self.cache_dir = Path(cache_dir)
        self.max_workers = max_workers

        # Configure mirror endpoint
        if use_mirror:
            os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
            print("ä½¿ç”¨ HuggingFace é•œåƒç«™: https://hf-mirror.com")

        # Create required directories
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def _get_checkpoint_path(self, dataset_name: str, subset_name: str) -> Path:
        """Get checkpoint file path"""
        checkpoint_id = hashlib.md5(f"{dataset_name}_{subset_name}".encode()).hexdigest()[:8]
        return self.cache_dir / f"checkpoint_{checkpoint_id}.pkl"

    def _load_checkpoint(self, checkpoint_path: Path) -> Optional[int]:
        """Load checkpoint and return the downloaded count"""
        if checkpoint_path.exists():
            try:
                with open(checkpoint_path, 'rb') as f:
                    checkpoint = pickle.load(f)
                downloaded_count = checkpoint.get('downloaded_count', 0)
                print(f"ä»æ£€æŸ¥ç‚¹æ¢å¤: å·²ä¸‹è½½ {downloaded_count} æ¡")
                return downloaded_count
            except Exception as e:
                print(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                return 0
        return 0

    def _save_checkpoint(self, checkpoint_path: Path, downloaded_count: int):
        """Save checkpoint"""
        try:
            checkpoint_data = {'downloaded_count': downloaded_count}
            with open(checkpoint_path, 'wb') as f:
                pickle.dump(checkpoint_data, f)
        except Exception as e:
            print(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

    def download_top_n(
        self,
        dataset_name: str,
        subset_name: str,
        target_count: int = 5000000,
        batch_size: int = 5000  # Increase batch size to reduce I/O
    ) -> str:
        """
        Directly download the first N samples (fastest approach)
        """
        output_path = self.output_dir / f"{subset_name}_top{target_count//1000000}M.jsonl"
        checkpoint_path = self._get_checkpoint_path(dataset_name, subset_name)

        # Check if already complete
        if output_path.exists():
            with open(output_path, 'r', encoding='utf-8') as f:
                existing_count = sum(1 for _ in f)
            if existing_count >= target_count:
                print(f"{subset_name} å·²å­˜åœ¨å®Œæ•´æ•°æ® ({existing_count} æ¡)ï¼Œè·³è¿‡ä¸‹è½½")
                return str(output_path)

        # Load checkpoint
        downloaded_count = self._load_checkpoint(checkpoint_path)

        print(f"æ­£åœ¨ä¸‹è½½ {dataset_name} çš„ {subset_name} å­é›†å‰ {target_count} æ¡æ•°æ®...")

        try:
            # Stream-load dataset
            dataset = load_dataset(
                dataset_name,
                subset_name,
                split="train",
                streaming=True
            )

            # If resuming, skip already downloaded samples
            if downloaded_count > 0:
                print(f"è·³è¿‡å‰ {downloaded_count} æ¡å·²ä¸‹è½½çš„æ•°æ®...")
                dataset = dataset.skip(downloaded_count)
                remaining_count = target_count - downloaded_count
            else:
                remaining_count = target_count

            # Open file for append write
            mode = 'a' if downloaded_count > 0 else 'w'
            with open(output_path, mode, encoding="utf-8") as f:
                batch_buffer = []

                with tqdm(
                    total=remaining_count,
                    initial=0,
                    desc=f"ä¸‹è½½ {subset_name}",
                    unit="æ¡"
                ) as pbar:

                    for idx, item in enumerate(dataset):
                        if idx >= remaining_count:
                            break

                        # Simplify data structure to reduce processing overhead
                        data_item = {
                            "text": item.get("text", ""),
                            "source": subset_name,
                            "idx": downloaded_count + idx
                        }

                        batch_buffer.append(data_item)

                        # Batch write to reduce I/O overhead
                        if len(batch_buffer) >= batch_size:
                            for data in batch_buffer:
                                f.write(json.dumps(data, ensure_ascii=False) + "\n")
                            f.flush()  # Ensure data is flushed to disk

                            # Update progress
                            current_downloaded = downloaded_count + idx + 1
                            pbar.update(len(batch_buffer))
                            pbar.set_postfix({
                                'downloaded': f"{current_downloaded:,}",
                                'speed': f"{len(batch_buffer)/(time.time() - pbar.last_print_t if pbar.last_print_t else 1):.0f}/s"
                            })

                            # Save checkpoint
                            self._save_checkpoint(checkpoint_path, current_downloaded)
                            batch_buffer = []

                    # Write remaining data
                    if batch_buffer:
                        for data in batch_buffer:
                            f.write(json.dumps(data, ensure_ascii=False) + "\n")
                        pbar.update(len(batch_buffer))
                        final_count = downloaded_count + len(batch_buffer) + (idx + 1 - len(batch_buffer))
                        self._save_checkpoint(checkpoint_path, final_count)

        except KeyboardInterrupt:
            print(f"\nä¸‹è½½è¢«ä¸­æ–­ï¼Œå½“å‰è¿›åº¦å·²ä¿å­˜åˆ°æ£€æŸ¥ç‚¹")
            return None
        except Exception as e:
            print(f"ä¸‹è½½å‡ºé”™: {e}")
            return None

        # Validate download results
        with open(output_path, 'r', encoding='utf-8') as f:
            actual_count = sum(1 for _ in f)

        print(f"ä¸‹è½½å®Œæˆ: {actual_count:,} æ¡æ•°æ®ä¿å­˜åˆ° {output_path}")

        # Cleanup checkpoint
        if checkpoint_path.exists():
            checkpoint_path.unlink()
            print("æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶")

        return str(output_path)

    def download_with_limit(
        self,
        dataset_name: str,
        subset_name: str,
        target_count: int = 5000000,
        skip_count: int = 0,  # Can skip the first N samples
        batch_size: int = 5000
    ) -> str:
        """
        Download a specific range: (skip_count, skip_count + target_count)
        """
        output_path = self.output_dir / f"{subset_name}_skip{skip_count//1000000}M_take{target_count//1000000}M.jsonl"

        print(f"æ­£åœ¨ä¸‹è½½ {dataset_name} çš„ {subset_name} å­é›†")
        print(f"è·³è¿‡å‰ {skip_count:,} æ¡ï¼Œä¸‹è½½æ¥ä¸‹æ¥çš„ {target_count:,} æ¡")

        try:
            dataset = load_dataset(
                dataset_name,
                subset_name,
                split="train",
                streaming=True
            )

            # Skip a specified number of samples
            if skip_count > 0:
                print(f"è·³è¿‡å‰ {skip_count:,} æ¡æ•°æ®...")
                dataset = dataset.skip(skip_count)

            # Download data
            with open(output_path, 'w', encoding="utf-8") as f:
                batch_buffer = []

                with tqdm(total=target_count, desc=f"ä¸‹è½½ {subset_name}", unit="æ¡") as pbar:
                    for idx, item in enumerate(dataset):
                        if idx >= target_count:
                            break

                        data_item = {
                            "text": item.get("text", ""),
                            "source": subset_name,
                            "idx": skip_count + idx
                        }

                        batch_buffer.append(data_item)

                        if len(batch_buffer) >= batch_size:
                            for data in batch_buffer:
                                f.write(json.dumps(data, ensure_ascii=False) + "\n")
                            f.flush()

                            pbar.update(len(batch_buffer))
                            batch_buffer = []

                    # Write remaining data
                    if batch_buffer:
                        for data in batch_buffer:
                            f.write(json.dumps(data, ensure_ascii=False) + "\n")
                        pbar.update(len(batch_buffer))

        except Exception as e:
            print(f"ä¸‹è½½å‡ºé”™: {e}")
            return None

        print(f"ä¸‹è½½å®Œæˆ: {output_path}")
        return str(output_path)

    def parallel_download(
        self,
        datasets_to_download: List[tuple],
        target_count: int = 5000000
    ) -> List[str]:
        """
        Download multiple datasets in parallel
        """
        output_paths = []

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_dataset = {
                executor.submit(
                    self.download_top_n,
                    dataset_name,
                    subset_name,
                    target_count
                ): (dataset_name, subset_name)
                for dataset_name, subset_name in datasets_to_download
            }

            for future in as_completed(future_to_dataset):
                dataset_name, subset_name = future_to_dataset[future]
                try:
                    output_path = future.result()
                    if output_path:
                        output_paths.append(output_path)
                        print(f"âœ… æˆåŠŸä¸‹è½½: {dataset_name}/{subset_name}")
                except Exception as e:
                    print(f"âŒ ä¸‹è½½å¤±è´¥ {dataset_name}/{subset_name}: {e}")

        return output_paths

    def estimate_download_time(
        self,
        dataset_name: str,
        subset_name: str,
        sample_size: int = 1000
    ):
        """
        Estimate download time
        """
        print(f"æ­£åœ¨æµ‹è¯• {dataset_name}/{subset_name} çš„ä¸‹è½½é€Ÿåº¦...")

        start_time = time.time()
        dataset = load_dataset(dataset_name, subset_name, split="train", streaming=True)

        count = 0
        for item in dataset:
            count += 1
            if count >= sample_size:
                break

        elapsed = time.time() - start_time
        speed = count / elapsed

        print(f"æµ‹è¯•ä¸‹è½½é€Ÿåº¦: {speed:.0f} æ¡/ç§’")

        # Estimate total time
        for target in [1000000, 5000000, 10000000]:
            estimated_time = target / speed
            print(f"ä¸‹è½½ {target:,} æ¡é¢„è®¡éœ€è¦: {estimated_time/60:.1f} åˆ†é’Ÿ")


def main():
    parser = argparse.ArgumentParser(description="è¶…å¿«æ•°æ®ä¸‹è½½è„šæœ¬")
    parser.add_argument("--target_count", type=int, default=5000000, help="ä¸‹è½½æ•°æ®æ¡æ•°")
    parser.add_argument("--output_dir", type=str, default="../../data/pretraining_test_data", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--cache_dir", type=str, default="../../data/download_cache", help="ç¼“å­˜ç›®å½•")
    parser.add_argument("--max_workers", type=int, default=3, help="æœ€å¤§å¹¶è¡Œä¸‹è½½æ•°")
    parser.add_argument("--batch_size", type=int, default=5000, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--sequential", action="store_true", help="é¡ºåºä¸‹è½½")
    parser.add_argument("--no_mirror", action="store_true", help="ä¸ä½¿ç”¨é•œåƒç«™")
    parser.add_argument("--estimate", action="store_true", help="ä¼°ç®—ä¸‹è½½æ—¶é—´")
    parser.add_argument("--skip_count", type=int, default=0, help="è·³è¿‡å‰Næ¡æ•°æ®")

    args = parser.parse_args()

    # Create downloader
    downloader = FastDataDownloader(
        output_dir=args.output_dir,
        cache_dir=args.cache_dir,
        max_workers=args.max_workers,
        use_mirror=not args.no_mirror
    )

    # Define datasets to download
    datasets_to_download = [
        # ("allenai/dolmino-mix-1124", "stackexchange"),
        ("allenai/olmo-mix-1124", "wiki"),
        ("allenai/olmo-mix-1124", "dclm")
    ]

    # If only estimating time
    if args.estimate:
        for dataset_name, subset_name in datasets_to_download:
            downloader.estimate_download_time(dataset_name, subset_name)
        return

    print(f"å¼€å§‹ä¸‹è½½ {len(datasets_to_download)} ä¸ªæ•°æ®é›†")
    print(f"æ¯ä¸ªæ•°æ®é›†ä¸‹è½½: {args.target_count:,} æ¡ (å‰{args.target_count//1000000}Mæ¡)")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"å¹¶è¡Œæ•°: {args.max_workers if not args.sequential else 1}")
    print(f"æ‰¹å¤„ç†å¤§å°: {args.batch_size}")
    print("=" * 60)

    start_time = time.time()

    if args.sequential:
        # Sequential download
        for dataset_name, subset_name in datasets_to_download:
            if args.skip_count > 0:
                downloader.download_with_limit(
                    dataset_name, subset_name, args.target_count, args.skip_count
                )
            else:
                downloader.download_top_n(
                    dataset_name, subset_name, args.target_count, args.batch_size
                )
    else:
        # Parallel download
        downloader.parallel_download(datasets_to_download, args.target_count)

    elapsed_time = time.time() - start_time
    total_data = args.target_count * len(datasets_to_download)

    print(f"\nğŸ‰ ä¸‹è½½å®Œæˆ!")
    print(f"æ€»è€—æ—¶: {elapsed_time:.1f} ç§’ ({elapsed_time/60:.1f} åˆ†é’Ÿ)")
    print(f"æ€»æ•°æ®é‡: {total_data:,} æ¡")
    print(f"å¹³å‡é€Ÿåº¦: {total_data/elapsed_time:.0f} æ¡/ç§’")
    print(f"å¹³å‡æ¯ä¸ªæ•°æ®é›†: {elapsed_time/len(datasets_to_download):.1f} ç§’")


if __name__ == "__main__":
    main()
