"""
åˆ†ææŒ‡ä»¤å¾®è°ƒæ•°æ®çš„ç‰¹å¾ï¼ˆæ›´æ–°ç‰ˆï¼‰
åŒ…å«æ›´æ–°çš„æŒ‡ä»¤è¯åˆ—è¡¨
"""
import json
import os
import re
import argparse
from typing import List, Dict, Set, Tuple
from collections import Counter, defaultdict
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import pandas as pd

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class InstructionAnalyzer:
    def __init__(self, instruction_verbs: Set[str] = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            instruction_verbs: æŒ‡ä»¤åŠ¨è¯é›†åˆï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é›†åˆ
        """
        self.analysis_functions = []

        # è®¾ç½®æŒ‡ä»¤åŠ¨è¯é›†åˆ
        if instruction_verbs is None:
            # é»˜è®¤çš„æŒ‡ä»¤åŠ¨è¯ï¼ˆåˆå¹¶ç”¨æˆ·æä¾›çš„åˆ—è¡¨ï¼‰
            instruct1 = [
                "translate", "explain", "summarize", "retrieve",
                "revise", 'generate', 'describe', 'classify', 'create',
                "evaluate", "correct", "develop",
                "identify", "analyze", "compose", "demonstrate", "interpret",
                "design", "solve", "follow", "clarify", "say", "help", "act",
                "recommend", "estimate", "edit", "format", "repeat"
            ]

            instruct2 = [
                "write", "give", "find", "create", "make", "describe", "design",
                "generate", "classify", "have", "explain", "tell", "identify",
                "output", "predict", "detect"
            ]

            # åˆå¹¶å¹¶å»é‡ï¼ˆè½¬ä¸ºå°å†™ï¼‰
            self.instruction_verbs = set(word.lower() for word in instruct1 + instruct2)
        else:
            self.instruction_verbs = set(word.lower() for word in instruction_verbs)

        print(f"å·²åŠ è½½ {len(self.instruction_verbs)} ä¸ªæŒ‡ä»¤åŠ¨è¯")

    def add_analysis(self, func, name: str):
        """
        æ·»åŠ åˆ†æå‡½æ•°

        Args:
            func: åˆ†æå‡½æ•°
            name: åˆ†æåç§°
        """
        self.analysis_functions.append((func, name))

    def count_words(self, text: str) -> int:
        """
        å¿«é€Ÿè¯æ•°ç»Ÿè®¡ï¼Œæ›¿ä»£tokenizer

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            è¯æ•°
        """
        # ç®€å•çš„è¯æ•°ç»Ÿè®¡ï¼ŒæŒ‰ç©ºæ ¼åˆ†å‰²
        return len(text.split())

    def count_characters(self, text: str) -> int:
        """
        å­—ç¬¦æ•°ç»Ÿè®¡

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            å­—ç¬¦æ•°
        """
        return len(text.strip())

    def analyze_length_distribution(self, data: List[Dict]) -> Dict:
        """
        åˆ†æé•¿åº¦åˆ†å¸ƒ

        ç›®çš„ï¼š
        - äº†è§£æŒ‡ä»¤å’Œå›ç­”çš„å…¸å‹é•¿åº¦
        - ä¸ºåç»­ç­›é€‰é¢„è®­ç»ƒæ•°æ®æä¾›å‚è€ƒ

        Args:
            data: æ•°æ®åˆ—è¡¨

        Returns:
            é•¿åº¦åˆ†å¸ƒç»Ÿè®¡å­—å…¸
        """
        instruction_word_lengths = []
        response_word_lengths = []
        total_word_lengths = []

        instruction_char_lengths = []
        response_char_lengths = []
        total_char_lengths = []

        for item in data:
            # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
            if 'messages' in item:
                # å¤„ç†messagesæ ¼å¼
                instruction_text = ""
                response_text = ""
                for msg in item['messages']:
                    if msg['role'] == 'user':
                        instruction_text += msg['content'] + " "
                    elif msg['role'] == 'assistant':
                        response_text += msg['content'] + " "
            else:
                # å¤„ç†ç›´æ¥æ ¼å¼
                instruction_text = item.get("instruction", item.get("instruction_text", ""))
                response_text = item.get("response", item.get("response_text", ""))

            # è¯æ•°ç»Ÿè®¡
            inst_words = self.count_words(instruction_text)
            resp_words = self.count_words(response_text)

            # å­—ç¬¦æ•°ç»Ÿè®¡
            inst_chars = self.count_characters(instruction_text)
            resp_chars = self.count_characters(response_text)

            instruction_word_lengths.append(inst_words)
            response_word_lengths.append(resp_words)
            total_word_lengths.append(inst_words + resp_words)

            instruction_char_lengths.append(inst_chars)
            response_char_lengths.append(resp_chars)
            total_char_lengths.append(inst_chars + resp_chars)

        return {
            "words": {
                "instruction": {
                    "mean": np.mean(instruction_word_lengths),
                    "median": np.median(instruction_word_lengths),
                    "std": np.std(instruction_word_lengths),
                    "percentiles": np.percentile(instruction_word_lengths, [10, 25, 50, 75, 90]).tolist(),
                    "max": np.max(instruction_word_lengths),
                    "min": np.min(instruction_word_lengths)
                },
                "response": {
                    "mean": np.mean(response_word_lengths),
                    "median": np.median(response_word_lengths),
                    "std": np.std(response_word_lengths),
                    "percentiles": np.percentile(response_word_lengths, [10, 25, 50, 75, 90]).tolist(),
                    "max": np.max(response_word_lengths),
                    "min": np.min(response_word_lengths)
                },
                "total": {
                    "mean": np.mean(total_word_lengths),
                    "median": np.median(total_word_lengths),
                    "percentiles": np.percentile(total_word_lengths, [10, 25, 50, 75, 90]).tolist()
                }
            },
            "characters": {
                "instruction": {
                    "mean": np.mean(instruction_char_lengths),
                    "median": np.median(instruction_char_lengths),
                    "std": np.std(instruction_char_lengths),
                    "percentiles": np.percentile(instruction_char_lengths, [10, 25, 50, 75, 90]).tolist()
                },
                "response": {
                    "mean": np.mean(response_char_lengths),
                    "median": np.median(response_char_lengths),
                    "std": np.std(response_char_lengths),
                    "percentiles": np.percentile(response_char_lengths, [10, 25, 50, 75, 90]).tolist()
                }
            },
            "raw_data": {
                "instruction_words": instruction_word_lengths,
                "response_words": response_word_lengths,
                "instruction_chars": instruction_char_lengths,
                "response_chars": response_char_lengths
            }
        }

    def extract_text_from_item(self, item: Dict) -> Tuple[str, str]:
        """
        ä»æ•°æ®é¡¹ä¸­æå–æŒ‡ä»¤å’Œå›ç­”æ–‡æœ¬

        Args:
            item: æ•°æ®é¡¹å­—å…¸

        Returns:
            (instruction, response) å…ƒç»„
        """
        if 'messages' in item:
            # å¤„ç†messagesæ ¼å¼
            instruction_text = ""
            response_text = ""
            for msg in item['messages']:
                if msg['role'] == 'user':
                    instruction_text += msg['content'] + " "
                elif msg['role'] == 'assistant':
                    response_text += msg['content'] + " "
        else:
            # å¤„ç†ç›´æ¥æ ¼å¼
            instruction_text = item.get("instruction", item.get("instruction_text", ""))
            response_text = item.get("response", item.get("response_text", ""))

        return instruction_text.strip(), response_text.strip()

    def analyze_instruction_patterns(self, data: List[Dict]) -> Dict:
        """
        åˆ†ææŒ‡ä»¤æ¨¡å¼

        Args:
            data: æ•°æ®åˆ—è¡¨

        Returns:
            æŒ‡ä»¤æ¨¡å¼ç»Ÿè®¡å­—å…¸
        """
        # æŒ‡ä»¤å¼€å¤´è¯ç»Ÿè®¡
        start_words = Counter()

        # æŒ‡ä»¤ç±»å‹æ¨¡å¼
        patterns = {
            "question": 0,  # ç–‘é—®å¥
            "command": 0,  # å‘½ä»¤å¥
            "completion": 0,  # è¡¥å…¨ä»»åŠ¡
            "generation": 0,  # ç”Ÿæˆä»»åŠ¡
            "explanation": 0,  # è§£é‡Šä»»åŠ¡
            "translation": 0,  # ç¿»è¯‘ä»»åŠ¡
            "summarization": 0,  # æ€»ç»“ä»»åŠ¡
            "analysis": 0,  # åˆ†æä»»åŠ¡
            "coding": 0,  # ç¼–ç¨‹ä»»åŠ¡
            "math": 0,  # æ•°å­¦ä»»åŠ¡
        }

        # å¸¸è§æŒ‡ä»¤åŠ¨è¯ï¼ˆä½¿ç”¨ç±»åˆå§‹åŒ–æ—¶è®¾ç½®çš„æŒ‡ä»¤åŠ¨è¯é›†åˆï¼‰
        instruction_verbs_counter = Counter()

        for item in data:
            instruction, _ = self.extract_text_from_item(item)
            instruction = instruction.lower()

            if not instruction:
                continue

            # ç»Ÿè®¡å¼€å¤´è¯
            words = instruction.split()
            if words:
                start_words[words[0]] += 1

            # è¯†åˆ«æŒ‡ä»¤ç±»å‹
            if "?" in instruction or any(q in instruction for q in ["what", "why", "how", "when", "where", "who"]):
                patterns["question"] += 1

            if any(cmd in instruction for cmd in ["write", "create", "generate", "make", "produce"]):
                patterns["generation"] += 1

            if any(exp in instruction for exp in ["explain", "describe", "elaborate", "define"]):
                patterns["explanation"] += 1

            if "complete" in instruction or "continue" in instruction or "finish" in instruction:
                patterns["completion"] += 1

            if "translate" in instruction or "translation" in instruction:
                patterns["translation"] += 1

            if any(sum_word in instruction for sum_word in ["summarize", "summary", "brief", "outline"]):
                patterns["summarization"] += 1

            if any(ana in instruction for ana in ["analyze", "analysis", "evaluate", "assess", "compare"]):
                patterns["analysis"] += 1

            if any(code in instruction for code in ["code", "function", "program", "script", "algorithm"]):
                patterns["coding"] += 1

            if any(math in instruction for math in ["calculate", "solve", "equation", "formula", "math"]):
                patterns["math"] += 1

            # æå–åŠ¨è¯ï¼ˆä½¿ç”¨self.instruction_verbsï¼‰
            for word in words:
                if word in self.instruction_verbs:
                    instruction_verbs_counter[word] += 1

        # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        total = len(data)
        patterns_pct = {k: (v / total) * 100 for k, v in patterns.items()}

        return {
            "start_words": dict(start_words.most_common(20)),
            "patterns": patterns_pct,
            "instruction_verbs": dict(instruction_verbs_counter.most_common(20))
        }

    def analyze_response_patterns(self, data: List[Dict]) -> Dict:
        """
        åˆ†æå›ç­”æ¨¡å¼

        Args:
            data: æ•°æ®åˆ—è¡¨

        Returns:
            å›ç­”æ¨¡å¼ç»Ÿè®¡å­—å…¸
        """
        response_formats = {
            "numbered_list": 0,  # ç¼–å·åˆ—è¡¨
            "bullet_points": 0,  # é¡¹ç›®ç¬¦å·
            "step_by_step": 0,  # åˆ†æ­¥éª¤
            "code_block": 0,  # ä»£ç å—
            "single_paragraph": 0,  # å•æ®µè½
            "multi_paragraph": 0,  # å¤šæ®µè½
            "here_is": 0,  # "Here is"å¼€å¤´
            "conversational": 0,  # å¯¹è¯å¼
            "structured": 0,  # ç»“æ„åŒ–å›ç­”
        }

        # å›ç­”å¼€å¤´çŸ­è¯­
        start_phrases = Counter()

        for item in data:
            _, response = self.extract_text_from_item(item)
            if not response:
                continue

            response_lower = response.lower()

            # æ£€æŸ¥æ ¼å¼
            if re.search(r'^\d+\.', response, re.MULTILINE):
                response_formats["numbered_list"] += 1

            if re.search(r'^[\*\-\â€¢]', response, re.MULTILINE):
                response_formats["bullet_points"] += 1

            if re.search(r'step \d|first.*then|next.*step', response_lower):
                response_formats["step_by_step"] += 1

            if "```" in response or re.search(r'`[^`]+`', response):
                response_formats["code_block"] += 1

            if (response_lower.startswith("here is") or response_lower.startswith("here's") or
                response_lower.startswith("here are")):
                response_formats["here_is"] += 1

            if any(conv in response_lower for conv in ["i think", "in my opinion", "i believe", "i would"]):
                response_formats["conversational"] += 1

            if re.search(r'(first|second|third|finally|in conclusion|therefore)', response_lower):
                response_formats["structured"] += 1

            # ç»Ÿè®¡æ®µè½æ•°
            paragraphs = [p for p in response.split('\n\n') if p.strip()]
            if len(paragraphs) == 1:
                response_formats["single_paragraph"] += 1
            elif len(paragraphs) > 1:
                response_formats["multi_paragraph"] += 1

            # æå–å¼€å¤´çŸ­è¯­ï¼ˆå‰5ä¸ªè¯ï¼‰
            words = response.split()[:5]
            if len(words) >= 2:
                phrase = " ".join(words[:2]).lower()
                start_phrases[phrase] += 1

        # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        total = len(data)
        formats_pct = {k: (v / total) * 100 for k, v in response_formats.items()}

        return {
            "formats": formats_pct,
            "start_phrases": dict(start_phrases.most_common(20))
        }

    def analyze_topic_distribution(self, data: List[Dict]) -> Dict:
        """
        åˆ†æä¸»é¢˜åˆ†å¸ƒ

        Args:
            data: æ•°æ®åˆ—è¡¨

        Returns:
            ä¸»é¢˜åˆ†å¸ƒå­—å…¸ï¼ˆç™¾åˆ†æ¯”å½¢å¼ï¼‰
        """
        # æ›´è¯¦ç»†çš„ä¸»é¢˜å…³é”®è¯åŒ¹é…
        topics = {
            "ç¼–ç¨‹å¼€å‘": ["code", "function", "program", "python", "javascript", "algorithm", "software", "debug", "api"],
            "æ•°å­¦è®¡ç®—": ["calculate", "solve", "equation", "number", "mathematical", "formula", "statistics"],
            "ç§‘å­¦ç ”ç©¶": ["scientific", "research", "experiment", "hypothesis", "theory", "data", "analysis"],
            "å†™ä½œåˆ›ä½œ": ["write", "essay", "story", "paragraph", "article", "creative", "poem"],
            "è¯­è¨€å­¦ä¹ ": ["translate", "grammar", "sentence", "word", "language", "english", "chinese"],
            "å¸¸è¯†é—®ç­”": ["explain", "what is", "define", "describe", "general knowledge"],
            "å•†ä¸šé‡‘è": ["business", "market", "finance", "economic", "investment", "strategy"],
            "æ•™è‚²å­¦ä¹ ": ["learn", "study", "education", "teach", "tutorial", "lesson"],
            "æŠ€æœ¯æ”¯æŒ": ["help", "how to", "troubleshoot", "fix", "setup", "configure"],
            "å¨±ä¹ä¼‘é—²": ["game", "movie", "music", "entertainment", "fun", "hobby"]
        }

        topic_counts = defaultdict(int)

        for item in data:
            instruction, response = self.extract_text_from_item(item)
            text = (instruction + " " + response).lower()

            for topic, keywords in topics.items():
                if any(keyword in text for keyword in keywords):
                    topic_counts[topic] += 1

        # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        total = len(data)
        topic_pct = {k: (v / total) * 100 for k, v in topic_counts.items()}

        return topic_pct

    def create_visualizations(self, results: Dict, output_dir: str):
        """
        åˆ›å»ºå¯è§†åŒ–å›¾è¡¨

        Args:
            results: åˆ†æç»“æœå­—å…¸
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        """
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('default')
        fig_size = (12, 8)

        # 1. é•¿åº¦åˆ†å¸ƒç›´æ–¹å›¾
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

        # æŒ‡ä»¤è¯æ•°åˆ†å¸ƒ
        inst_words = results['length_distribution']['raw_data']['instruction_words']
        ax1.hist(inst_words, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        ax1.set_title('æŒ‡ä»¤é•¿åº¦åˆ†å¸ƒ (è¯æ•°)', fontsize=14, fontweight='bold')
        ax1.set_xlabel('è¯æ•°')
        ax1.set_ylabel('é¢‘æ¬¡')
        ax1.axvline(np.mean(inst_words), color='red', linestyle='--', label=f'å¹³å‡å€¼: {np.mean(inst_words):.1f}')
        ax1.legend()

        # å›ç­”è¯æ•°åˆ†å¸ƒ
        resp_words = results['length_distribution']['raw_data']['response_words']
        ax2.hist(resp_words, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')
        ax2.set_title('å›ç­”é•¿åº¦åˆ†å¸ƒ (è¯æ•°)', fontsize=14, fontweight='bold')
        ax2.set_xlabel('è¯æ•°')
        ax2.set_ylabel('é¢‘æ¬¡')
        ax2.axvline(np.mean(resp_words), color='red', linestyle='--', label=f'å¹³å‡å€¼: {np.mean(resp_words):.1f}')
        ax2.legend()

        # æŒ‡ä»¤å­—ç¬¦æ•°åˆ†å¸ƒ
        inst_chars = results['length_distribution']['raw_data']['instruction_chars']
        ax3.hist(inst_chars, bins=50, alpha=0.7, color='orange', edgecolor='black')
        ax3.set_title('æŒ‡ä»¤é•¿åº¦åˆ†å¸ƒ (å­—ç¬¦æ•°)', fontsize=14, fontweight='bold')
        ax3.set_xlabel('å­—ç¬¦æ•°')
        ax3.set_ylabel('é¢‘æ¬¡')
        ax3.axvline(np.mean(inst_chars), color='red', linestyle='--', label=f'å¹³å‡å€¼: {np.mean(inst_chars):.1f}')
        ax3.legend()

        # å›ç­”å­—ç¬¦æ•°åˆ†å¸ƒ
        resp_chars = results['length_distribution']['raw_data']['response_chars']
        ax4.hist(resp_chars, bins=50, alpha=0.7, color='pink', edgecolor='black')
        ax4.set_title('å›ç­”é•¿åº¦åˆ†å¸ƒ (å­—ç¬¦æ•°)', fontsize=14, fontweight='bold')
        ax4.set_xlabel('å­—ç¬¦æ•°')
        ax4.set_ylabel('é¢‘æ¬¡')
        ax4.axvline(np.mean(resp_chars), color='red', linestyle='--', label=f'å¹³å‡å€¼: {np.mean(resp_chars):.1f}')
        ax4.legend()

        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'length_distribution.png'), dpi=300, bbox_inches='tight')
        plt.close()

        # 2. æŒ‡ä»¤ç±»å‹åˆ†å¸ƒé¥¼å›¾
        patterns = results['instruction_patterns']['patterns']
        patterns_filtered = {k: v for k, v in patterns.items() if v > 1}  # åªæ˜¾ç¤ºå¤§äº1%çš„

        fig, ax = plt.subplots(figsize=fig_size)
        colors = plt.cm.Set3(np.linspace(0, 1, len(patterns_filtered)))
        wedges, texts, autotexts = ax.pie(patterns_filtered.values(),
                                         labels=patterns_filtered.keys(),
                                         autopct='%1.1f%%',
                                         colors=colors,
                                         startangle=90)
        ax.set_title('æŒ‡ä»¤ç±»å‹åˆ†å¸ƒ', fontsize=16, fontweight='bold')
        plt.setp(autotexts, size=10, weight="bold")
        plt.savefig(os.path.join(output_dir, 'instruction_patterns.png'), dpi=300, bbox_inches='tight')
        plt.close()

        # 3. å›ç­”æ ¼å¼åˆ†å¸ƒæ¡å½¢å›¾
        formats = results['response_patterns']['formats']
        formats_filtered = {k: v for k, v in formats.items() if v > 1}

        fig, ax = plt.subplots(figsize=fig_size)
        bars = ax.barh(list(formats_filtered.keys()), list(formats_filtered.values()),
                       color='lightcoral', edgecolor='black')
        ax.set_title('å›ç­”æ ¼å¼åˆ†å¸ƒ', fontsize=16, fontweight='bold')
        ax.set_xlabel('ç™¾åˆ†æ¯” (%)')

        # åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars:
            width = bar.get_width()
            ax.text(width, bar.get_y() + bar.get_height()/2,
                   f'{width:.1f}%', ha='left', va='center', fontweight='bold')

        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'response_formats.png'), dpi=300, bbox_inches='tight')
        plt.close()

        # 4. ä¸»é¢˜åˆ†å¸ƒæ¡å½¢å›¾
        topics = results['topic_distribution']
        topics_filtered = {k: v for k, v in topics.items() if v > 0.5}

        fig, ax = plt.subplots(figsize=fig_size)
        bars = ax.bar(list(topics_filtered.keys()), list(topics_filtered.values()),
                      color='lightblue', edgecolor='black')
        ax.set_title('ä¸»é¢˜åˆ†å¸ƒ', fontsize=16, fontweight='bold')
        ax.set_ylabel('ç™¾åˆ†æ¯” (%)')
        ax.set_xlabel('ä¸»é¢˜ç±»åˆ«')
        plt.xticks(rotation=45, ha='right')

        # åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')

        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'topic_distribution.png'), dpi=300, bbox_inches='tight')
        plt.close()

        # 5. å¸¸ç”¨æŒ‡ä»¤åŠ¨è¯è¯äº‘é£æ ¼æ¡å½¢å›¾
        verbs = results['instruction_patterns']['instruction_verbs']
        top_verbs = dict(list(verbs.items())[:15])

        fig, ax = plt.subplots(figsize=(10, 8))
        bars = ax.barh(list(top_verbs.keys()), list(top_verbs.values()),
                       color='gold', edgecolor='black')
        ax.set_title('æœ€å¸¸ç”¨æŒ‡ä»¤åŠ¨è¯ (Top 15)', fontsize=16, fontweight='bold')
        ax.set_xlabel('å‡ºç°æ¬¡æ•°')

        for bar in bars:
            width = bar.get_width()
            ax.text(width, bar.get_y() + bar.get_height()/2,
                   f'{int(width)}', ha='left', va='center', fontweight='bold')

        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'instruction_verbs.png'), dpi=300, bbox_inches='tight')
        plt.close()

        print(f"æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {output_dir}")

    def run_analysis(self, data: List[Dict]) -> Dict:
        """
        è¿è¡Œæ‰€æœ‰åˆ†æ

        Args:
            data: æ•°æ®åˆ—è¡¨

        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        results = {}

        # æ·»åŠ æ‰€æœ‰åˆ†æå‡½æ•°
        self.add_analysis(self.analyze_length_distribution, "length_distribution")
        self.add_analysis(self.analyze_instruction_patterns, "instruction_patterns")
        self.add_analysis(self.analyze_response_patterns, "response_patterns")
        self.add_analysis(self.analyze_topic_distribution, "topic_distribution")

        # æ‰§è¡Œåˆ†æ
        for func, name in self.analysis_functions:
            print(f"æ‰§è¡Œåˆ†æ: {name}")
            results[name] = func(data)

        return results


def main():
    """
    ä¸»å‡½æ•°ï¼šè§£æå‚æ•°å¹¶æ‰§è¡Œåˆ†æ
    """
    parser = argparse.ArgumentParser(description="åˆ†ææŒ‡ä»¤å¾®è°ƒæ•°æ®")
    parser.add_argument("--input_file", type=str,
                        default="/root/autodl-tmp/ift_memorization/data/instruction_test_data/olmo_instruction_tulu3_intersection.jsonl",
                        help="è¾“å…¥æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", type=str,
                        default="/root/autodl-tmp/ift_memorization/data/instruction_test_data/analysis",
                        help="è¾“å‡ºç›®å½•")
    parser.add_argument("--max_samples", type=int,
                        default=None,
                        help="æœ€å¤§åˆ†ææ ·æœ¬æ•°")

    args = parser.parse_args()

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.input_file):
        print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
        print("è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æˆ–è€…ä½¿ç”¨ --input_file æŒ‡å®šæ­£ç¡®çš„è·¯å¾„")
        return

    # åŠ è½½æ•°æ®
    print(f"åŠ è½½æ•°æ®: {args.input_file}")
    data = []
    try:
        with open(args.input_file, "r", encoding="utf-8") as f:
            for line_num, line in enumerate(f, 1):
                try:
                    data.append(json.loads(line))
                    if args.max_samples and len(data) >= args.max_samples:
                        break
                except json.JSONDecodeError:
                    print(f"è­¦å‘Š: ç¬¬{line_num}è¡ŒJSONæ ¼å¼é”™è¯¯ï¼Œè·³è¿‡")
                    continue
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return

    print(f"æˆåŠŸåŠ è½½ {len(data)} æ¡æ•°æ®")

    if len(data) == 0:
        print("é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä»¥åˆ†æ")
        return

    # åˆ›å»ºåˆ†æå™¨ï¼ˆä½¿ç”¨æ›´æ–°çš„æŒ‡ä»¤åŠ¨è¯åˆ—è¡¨ï¼‰
    analyzer = InstructionAnalyzer()

    # æ‰§è¡Œåˆ†æ
    results = analyzer.run_analysis(data)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)

    # ä¿å­˜ç»“æœ
    output_path = os.path.join(args.output_dir, "instruction_analysis.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    print(f"åˆ†æç»“æœä¿å­˜åˆ°: {output_path}")

    # åˆ›å»ºå¯è§†åŒ–
    print("åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
    analyzer.create_visualizations(results, args.output_dir)

    # æ‰“å°è¯¦ç»†æ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“Š æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†åˆ†ææŠ¥å‘Š")
    print("="*60)

    print(f"\nğŸ“ˆ åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»æ•°æ®é‡: {len(data):,} æ¡")
    print(f"  æŒ‡ä»¤å¹³å‡é•¿åº¦: {results['length_distribution']['words']['instruction']['mean']:.1f} è¯")
    print(f"  å›ç­”å¹³å‡é•¿åº¦: {results['length_distribution']['words']['response']['mean']:.1f} è¯")
    print(f"  æŒ‡ä»¤å¹³å‡å­—ç¬¦æ•°: {results['length_distribution']['characters']['instruction']['mean']:.0f}")
    print(f"  å›ç­”å¹³å‡å­—ç¬¦æ•°: {results['length_distribution']['characters']['response']['mean']:.0f}")

    print(f"\nğŸ¯ æŒ‡ä»¤ç±»å‹åˆ†å¸ƒ (>5%):")
    for pattern, pct in results['instruction_patterns']['patterns'].items():
        if pct > 5:
            print(f"  {pattern}: {pct:.1f}%")

    print(f"\nğŸ“ å›ç­”æ ¼å¼åˆ†å¸ƒ (>5%):")
    for format_type, pct in results['response_patterns']['formats'].items():
        if pct > 5:
            print(f"  {format_type}: {pct:.1f}%")

    print(f"\nğŸ·ï¸ ä¸»é¢˜åˆ†å¸ƒ (>2%):")
    for topic, pct in results['topic_distribution'].items():
        if pct > 2:
            print(f"  {topic}: {pct:.1f}%")

    print(f"\nğŸ”¤ æœ€å¸¸ç”¨æŒ‡ä»¤åŠ¨è¯ (Top 10):")
    for i, (verb, count) in enumerate(list(results['instruction_patterns']['instruction_verbs'].items())[:10], 1):
        print(f"  {i:2d}. {verb}: {count} æ¬¡")

    print(f"\nğŸ’¬ å¸¸è§å›ç­”å¼€å¤´ (Top 5):")
    for i, (phrase, count) in enumerate(list(results['response_patterns']['start_phrases'].items())[:5], 1):
        print(f"  {i}. \"{phrase}\": {count} æ¬¡")

    print(f"\nğŸ“Š æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ°: {args.output_dir}")
    print("="*60)


if __name__ == "__main__":
    main()