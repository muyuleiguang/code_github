#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import argparse
import os
import glob
import pandas as pd
from typing import Dict, List
import warnings
import numpy as np
import torch
from transformers import AutoTokenizer
from sentence_transformers import SentenceTransformer
from rouge import Rouge
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import editdistance

warnings.filterwarnings('ignore')


class MemorizationMetrics:
    def __init__(
            self,
            tokenizer_name: str = None,
            sentence_model_name: str = "/root/autodl-tmp/ift_memorization/model_cache/sentence_transformers"
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡

        Args:
            tokenizer_name: tokenizeræ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            sentence_model_name: å¥å­embeddingæ¨¡å‹åç§°
        """
        self.tokenizer = None
        if tokenizer_name:
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
            except:
                print(f"è­¦å‘Š: æ— æ³•åŠ è½½tokenizer {tokenizer_name}")

        try:
            self.sentence_model = SentenceTransformer(sentence_model_name)
        except Exception as e:
            print(f"è­¦å‘Š: åŠ è½½ sentence model '{sentence_model_name}' æ—¶å‘ç”Ÿé”™è¯¯ã€‚")
            self.sentence_model = None

        self.rouge = Rouge()
        self.smoothing = SmoothingFunction()

    def exact_match_rate(
            self,
            generated_tokens: List[List[int]],
            reference_tokens: List[List[int]]
    ) -> Dict[str, float]:
        """
        ç¬¬ä¸€ç§ï¼šç²¾ç¡®åŒ¹é…ç‡ (Exact Match Rate)

        æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ä¸è®­ç»ƒæ•°æ®åŸæ–‡å®Œå…¨ä¸€è‡´çš„æ¯”ä¾‹ã€‚è¿™æ˜¯æœ€ä¸¥æ ¼çš„æŒ‡æ ‡ã€‚

        Args:
            generated_tokens: ç”Ÿæˆçš„tokenåˆ—è¡¨
            reference_tokens: å‚è€ƒtokenåˆ—è¡¨

        Returns:
            ç²¾ç¡®åŒ¹é…æŒ‡æ ‡
        """
        assert len(generated_tokens) == len(reference_tokens)

        exact_matches = 0
        for gen_tokens, ref_tokens in zip(generated_tokens, reference_tokens):
            if gen_tokens == ref_tokens:
                exact_matches += 1

        return {
            "exact_match_rate": exact_matches / len(generated_tokens),
            "exact_matches": exact_matches,
            "total_samples": len(generated_tokens)
        }

    def rouge_bleu_scores(
            self,
            generated_texts: List[str],
            reference_texts: List[str],
            generated_tokens: List[List[int]] = None,
            reference_tokens: List[List[int]] = None
    ) -> Dict[str, float]:
        """
        ç¬¬äºŒç§ï¼šROUGE / BLEU åˆ†æ•°

        ç”¨äºè¡¡é‡ç”Ÿæˆæ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬ä¹‹é—´çš„n-gramé‡å åº¦ã€‚
        å¯ä»¥æ•æ‰åˆ°è¿‘ä¼¼è®°å¿†ï¼ˆnear-verbatim memorizationï¼‰ã€‚

        Args:
            generated_texts: ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
            reference_texts: å‚è€ƒæ–‡æœ¬åˆ—è¡¨
            generated_tokens: ç”Ÿæˆçš„tokenåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œç”¨äºtokençº§BLEUï¼‰
            reference_tokens: å‚è€ƒtokenåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œç”¨äºtokençº§BLEUï¼‰

        Returns:
            ROUGEå’ŒBLEUæŒ‡æ ‡
        """
        results = {}

        # ROUGE scores (åŸºäºæ–‡æœ¬)
        if generated_texts and reference_texts:
            assert len(generated_texts) == len(reference_texts)

            rouge_1_scores = []
            rouge_2_scores = []
            rouge_l_scores = []

            for gen, ref in zip(generated_texts, reference_texts):
                if not gen or not ref:
                    rouge_1_scores.append(0.0)
                    rouge_2_scores.append(0.0)
                    rouge_l_scores.append(0.0)
                    continue

                try:
                    scores = self.rouge.get_scores(gen, ref)[0]
                    rouge_1_scores.append(scores['rouge-1']['f'])
                    rouge_2_scores.append(scores['rouge-2']['f'])
                    rouge_l_scores.append(scores['rouge-l']['f'])
                except:
                    rouge_1_scores.append(0.0)
                    rouge_2_scores.append(0.0)
                    rouge_l_scores.append(0.0)

            results.update({
                "rouge_1_f": np.mean(rouge_1_scores),
                "rouge_2_f": np.mean(rouge_2_scores),
                "rouge_l_f": np.mean(rouge_l_scores),
                "rouge_1_std": np.std(rouge_1_scores),
                "rouge_2_std": np.std(rouge_2_scores),
                "rouge_l_std": np.std(rouge_l_scores)
            })

        # BLEU scores (åŸºäºtokenï¼Œæ›´ç²¾ç¡®)
        if generated_tokens and reference_tokens:
            assert len(generated_tokens) == len(reference_tokens)

            bleu_1_scores = []
            bleu_2_scores = []
            bleu_4_scores = []

            for gen_tokens, ref_tokens in zip(generated_tokens, reference_tokens):
                if not gen_tokens or not ref_tokens:
                    bleu_1_scores.append(0.0)
                    bleu_2_scores.append(0.0)
                    bleu_4_scores.append(0.0)
                    continue

                # å°†token IDè½¬æ¢ä¸ºå­—ç¬¦ä¸²ç”¨äºBLEUè®¡ç®—
                gen_str_tokens = [str(t) for t in gen_tokens]
                ref_str_tokens = [str(t) for t in ref_tokens]

                # BLEU-1
                bleu_1 = sentence_bleu([ref_str_tokens], gen_str_tokens,
                                       weights=(1, 0, 0, 0),
                                       smoothing_function=self.smoothing.method1)
                bleu_1_scores.append(bleu_1)

                # BLEU-2
                bleu_2 = sentence_bleu([ref_str_tokens], gen_str_tokens,
                                       weights=(0.5, 0.5, 0, 0),
                                       smoothing_function=self.smoothing.method1)
                bleu_2_scores.append(bleu_2)

                # BLEU-4
                bleu_4 = sentence_bleu([ref_str_tokens], gen_str_tokens,
                                       weights=(0.25, 0.25, 0.25, 0.25),
                                       smoothing_function=self.smoothing.method1)
                bleu_4_scores.append(bleu_4)

            results.update({
                "bleu_1": np.mean(bleu_1_scores),
                "bleu_2": np.mean(bleu_2_scores),
                "bleu_4": np.mean(bleu_4_scores),
                "bleu_1_std": np.std(bleu_1_scores),
                "bleu_2_std": np.std(bleu_2_scores),
                "bleu_4_std": np.std(bleu_4_scores)
            })

        return results

    def edit_distance_metrics(
            self,
            generated_tokens: List[List[int]],
            reference_tokens: List[List[int]],
            generated_texts: List[str] = None,
            reference_texts: List[str] = None
    ) -> Dict[str, float]:
        """
        ç¬¬ä¸‰ç§ï¼šç¼–è¾‘è·ç¦» (Edit Distance)

        ç”Ÿæˆæ–‡æœ¬éœ€è¦ç»è¿‡å¤šå°‘æ¬¡å¢ã€åˆ ã€æ”¹æ‰èƒ½å˜æˆåŸæ–‡ã€‚
        è·ç¦»è¶Šå°ï¼Œè®°å¿†ç¨‹åº¦è¶Šé«˜ã€‚ä¸»è¦ä½¿ç”¨Token-level Edit Distanceã€‚

        Args:
            generated_tokens: ç”Ÿæˆçš„tokenåˆ—è¡¨
            reference_tokens: å‚è€ƒtokenåˆ—è¡¨
            generated_texts: ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            reference_texts: å‚è€ƒæ–‡æœ¬åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            ç¼–è¾‘è·ç¦»æŒ‡æ ‡
        """
        assert len(generated_tokens) == len(reference_tokens)

        token_distances = []
        normalized_token_distances = []

        # Tokençº§ç¼–è¾‘è·ç¦»ï¼ˆä¸»è¦æŒ‡æ ‡ï¼‰
        for gen_tokens, ref_tokens in zip(generated_tokens, reference_tokens):
            token_dist = editdistance.eval(gen_tokens, ref_tokens)
            token_distances.append(token_dist)

            # å½’ä¸€åŒ–tokenç¼–è¾‘è·ç¦»
            max_token_len = max(len(gen_tokens), len(ref_tokens))
            if max_token_len > 0:
                normalized_token_distances.append(token_dist / max_token_len)
            else:
                normalized_token_distances.append(0.0)

        results = {
            "token_edit_distance": np.mean(token_distances),
            "token_edit_distance_std": np.std(token_distances),
            "normalized_token_edit_distance": np.mean(normalized_token_distances),
            "normalized_token_edit_distance_std": np.std(normalized_token_distances),
            "min_token_edit_distance": np.min(token_distances),
            "max_token_edit_distance": np.max(token_distances),
            "median_token_edit_distance": np.median(token_distances)
        }

        # å­—ç¬¦çº§ç¼–è¾‘è·ç¦»ï¼ˆå¦‚æœæä¾›äº†æ–‡æœ¬ï¼‰
        if generated_texts and reference_texts:
            char_distances = []
            normalized_char_distances = []

            for gen_text, ref_text in zip(generated_texts, reference_texts):
                char_dist = editdistance.eval(gen_text, ref_text)
                char_distances.append(char_dist)

                # å½’ä¸€åŒ–å­—ç¬¦ç¼–è¾‘è·ç¦»
                max_char_len = max(len(gen_text), len(ref_text))
                if max_char_len > 0:
                    normalized_char_distances.append(char_dist / max_char_len)
                else:
                    normalized_char_distances.append(0.0)

            results.update({
                "char_edit_distance": np.mean(char_distances),
                "char_edit_distance_std": np.std(char_distances),
                "normalized_char_edit_distance": np.mean(normalized_char_distances),
                "normalized_char_edit_distance_std": np.std(normalized_char_distances),
                "min_char_edit_distance": np.min(char_distances),
                "max_char_edit_distance": np.max(char_distances),
                "median_char_edit_distance": np.median(char_distances)
            })

        return results

    def semantic_similarity(
            self,
            generated_texts: List[str],
            reference_texts: List[str]
    ) -> Dict[str, float]:
        """
        ç¬¬å››ç§ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦

        ä½¿ç”¨SentenceBERTç­‰è®¡ç®—embeddingç›¸ä¼¼åº¦

        Args:
            generated_texts: ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
            reference_texts: å‚è€ƒæ–‡æœ¬åˆ—è¡¨

        Returns:
            è¯­ä¹‰ç›¸ä¼¼åº¦æŒ‡æ ‡
        """
        if self.sentence_model is None:
            return {
                "avg_similarity": 0.0,
                "similarity_std": 0.0,
                "min_similarity": 0.0,
                "max_similarity": 0.0,
                "median_similarity": 0.0
            }

    def likelihood_ppl_loss_metrics(
            self,
            top_tokens_list: List[List[List[Dict]]],
            reference_tokens: List[List[int]],
            logits=None
    ) -> Dict[str, float]:
        """
        ç¬¬äº”ç§ï¼šLikelihood, PPL, loss, logits

        åŸºäºæ¨¡å‹è¾“å‡ºæ¦‚ç‡è®¡ç®—è®°å¿†ç›¸å…³æŒ‡æ ‡

        Args:
            top_tokens_list: æ¯ä¸ªæ ·æœ¬çš„æ¯ä¸€æ­¥top-k tokenæ¦‚ç‡ä¿¡æ¯
                æ ¼å¼: [sample][step][top_k_tokens]
                æ¯ä¸ªtoken_infoåŒ…å«: {'token_id': int, 'probability': float, 'rank': int}
            reference_tokens: å‚è€ƒtokenåˆ—è¡¨
            logits: å®Œæ•´çš„logitså¼ é‡ï¼ˆå¯é€‰ï¼Œå¦‚æœæœ‰çš„è¯æ›´ç²¾ç¡®ï¼‰

        Returns:
            likelihood, perplexity, lossç›¸å…³æŒ‡æ ‡
        """
        if not top_tokens_list or not reference_tokens:
            return {
                "avg_log_likelihood": float('-inf'),
                "perplexity": float('inf'),
                "avg_loss": float('inf'),
                "target_token_probability": 0.0,
                "target_token_rank": float('inf'),
                "target_in_top1_rate": 0.0,
                "target_in_top3_rate": 0.0,
                "target_in_top5_rate": 0.0
            }

        log_likelihoods = []
        losses = []
        target_probs = []
        target_ranks = []
        top1_hits = 0
        top3_hits = 0
        top5_hits = 0
        total_positions = 0

        for sample_idx, (sample_top_tokens, ref_tokens) in enumerate(zip(top_tokens_list, reference_tokens)):
            if not sample_top_tokens or not ref_tokens:
                continue

            sample_log_likelihood = 0.0
            sample_positions = 0

            for step_idx, step_top_tokens in enumerate(sample_top_tokens):
                if step_idx >= len(ref_tokens):
                    break

                target_token = ref_tokens[step_idx]
                total_positions += 1
                sample_positions += 1

                # æŸ¥æ‰¾ç›®æ ‡tokenåœ¨top-kä¸­çš„ä½ç½®å’Œæ¦‚ç‡
                target_found = False

                for rank, token_info in enumerate(step_top_tokens):
                    token_id = token_info.get('token_id')
                    prob = token_info.get('probability', 0.0)

                    if token_id == target_token:
                        target_probs.append(prob)
                        target_ranks.append(rank + 1)  # rankä»1å¼€å§‹
                        target_found = True

                        # è®¡ç®—log likelihood
                        if prob > 0:
                            log_prob = np.log(prob)
                            sample_log_likelihood += log_prob

                            # è®¡ç®—cross-entropy loss
                            loss = -log_prob
                            losses.append(loss)
                        else:
                            sample_log_likelihood += -100  # é¿å…log(0)
                            losses.append(100)

                        # ç»Ÿè®¡top-kå‘½ä¸­ç‡
                        if rank == 0:  # top-1
                            top1_hits += 1
                        if rank < 3:  # top-3
                            top3_hits += 1
                        if rank < 5:  # top-5
                            top5_hits += 1
                        break

                if not target_found:
                    # ç›®æ ‡tokenä¸åœ¨top-kä¸­ï¼Œä½¿ç”¨å¾ˆå°çš„æ¦‚ç‡
                    target_probs.append(1e-10)
                    target_ranks.append(float('inf'))
                    sample_log_likelihood += -100
                    losses.append(100)

            if sample_positions > 0:
                log_likelihoods.append(sample_log_likelihood / sample_positions)

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_log_likelihood = np.mean(log_likelihoods) if log_likelihoods else float('-inf')
        perplexity = np.exp(-avg_log_likelihood) if avg_log_likelihood != float('-inf') else float('inf')
        avg_loss = np.mean(losses) if losses else float('inf')
        avg_target_prob = np.mean(target_probs) if target_probs else 0.0

        finite_ranks = [r for r in target_ranks if r != float('inf')]
        avg_target_rank = np.mean(finite_ranks) if finite_ranks else float('inf')

        return {
            "avg_log_likelihood": float(avg_log_likelihood),
            "perplexity": float(perplexity),
            "avg_loss": float(avg_loss),
            "target_token_probability": float(avg_target_prob),
            "target_token_rank": float(avg_target_rank),
            "target_in_top1_rate": top1_hits / total_positions if total_positions > 0 else 0.0,
            "target_in_top3_rate": top3_hits / total_positions if total_positions > 0 else 0.0,
            "target_in_top5_rate": top5_hits / total_positions if total_positions > 0 else 0.0,
            "target_prob_std": float(np.std(target_probs)) if target_probs else 0.0,
            "total_positions": total_positions
        }

    def compute_all_metrics_from_data(
            self,
            samples: List[Dict]
    ) -> Dict[str, Dict]:
        """
        ä»æ ·æœ¬æ•°æ®è®¡ç®—æ‰€æœ‰5ç§è¯„ä¼°æŒ‡æ ‡

        Args:
            samples: æ ·æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«generated_tokens, original_continuation_tokensç­‰å­—æ®µ

        Returns:
            åŒ…å«æ‰€æœ‰æŒ‡æ ‡ç»“æœçš„å­—å…¸
        """
        # æå–æ•°æ®
        generated_tokens = []
        reference_tokens = []
        generated_texts = []
        reference_texts = []
        top_tokens_list = []

        for sample in samples:
            if 'generated_tokens' in sample and 'original_continuation_tokens' in sample:
                generated_tokens.append(sample['generated_tokens'])
                reference_tokens.append(sample['original_continuation_tokens'])

            if 'generated_text' in sample and 'original_continuation' in sample:
                generated_texts.append(sample['generated_text'])
                reference_texts.append(sample['original_continuation'])

            if 'top_tokens' in sample:
                top_tokens_list.append(sample['top_tokens'])

        results = {}

        # ç¬¬ä¸€ç§ï¼šç²¾ç¡®åŒ¹é…ç‡
        if generated_tokens and reference_tokens:
            results["exact_match"] = self.exact_match_rate(generated_tokens, reference_tokens)

        # ç¬¬äºŒç§ï¼šROUGE/BLEUåˆ†æ•°
        if generated_texts and reference_texts:
            results["rouge_bleu"] = self.rouge_bleu_scores(
                generated_texts, reference_texts, generated_tokens, reference_tokens
            )

        # ç¬¬ä¸‰ç§ï¼šç¼–è¾‘è·ç¦»
        if generated_tokens and reference_tokens:
            results["edit_distance"] = self.edit_distance_metrics(
                generated_tokens, reference_tokens, generated_texts, reference_texts
            )

        # ç¬¬å››ç§ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦
        if generated_texts and reference_texts:
            results["semantic"] = self.semantic_similarity(generated_texts, reference_texts)

        # ç¬¬äº”ç§ï¼šLikelihood, PPL, loss
        if top_tokens_list and reference_tokens:
            results["likelihood"] = self.likelihood_ppl_loss_metrics(top_tokens_list, reference_tokens)

        return results


def sort_model_scales(model_scales):
    """
    æŒ‰ç…§æ¨¡å‹è§„æ¨¡æ•°å€¼å¤§å°æ’åºï¼Œå¦‚ 1B < 7B < 13B < 32B

    Args:
        model_scales: æ¨¡å‹è§„æ¨¡åˆ—è¡¨

    Returns:
        sorted_scales: æ’åºåçš„æ¨¡å‹è§„æ¨¡åˆ—è¡¨
    """

    def extract_scale_value(scale_str):
        """æå–æ¨¡å‹è§„æ¨¡çš„æ•°å€¼éƒ¨åˆ†ç”¨äºæ’åº"""
        try:
            # ç§»é™¤æœ«å°¾çš„å•ä½ï¼ˆBã€Mç­‰ï¼‰
            if scale_str.endswith('B'):
                return float(scale_str[:-1])
            elif scale_str.endswith('M'):
                return float(scale_str[:-1]) / 1000  # è½¬æ¢ä¸ºBå•ä½
            else:
                # å¦‚æœæ²¡æœ‰å•ä½ï¼Œç›´æ¥å½“ä½œæ•°å­—å¤„ç†
                return float(scale_str)
        except:
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªå¤§æ•°å€¼ï¼Œæ’åœ¨æœ€å
            return float('inf')

    return sorted(model_scales, key=extract_scale_value)


def load_generation_results_memory_optimized(results_base_dir: str,
                                             model_scales: List[str],
                                             datasets: List[str] = None,
                                             prefix_lengths: List[int] = None,
                                             generation_lengths: List[int] = None,
                                             max_samples: int = None) -> Dict[str, Dict]:
    """
    å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼šåˆ†æ‰¹åŠ è½½ç”Ÿæˆç»“æœæ–‡ä»¶ï¼Œå‡å°‘å†…å­˜å ç”¨

    Args:
        results_base_dir: ç»“æœåŸºç¡€ç›®å½•è·¯å¾„ï¼ŒåŒ…å«exp1_generation_Xå­æ–‡ä»¶å¤¹
        model_scales: æ¨¡å‹è§„æ¨¡åˆ—è¡¨ (å¦‚ ["1B", "7B", "13B"])
        datasets: è¦åŠ è½½çš„æ•°æ®é›†åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºåŠ è½½æ‰€æœ‰
        prefix_lengths: è¦åŠ è½½çš„å‰ç¼€é•¿åº¦åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºåŠ è½½æ‰€æœ‰
        generation_lengths: è¦åŠ è½½çš„ç”Ÿæˆé•¿åº¦åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºåŠ è½½æ‰€æœ‰
        max_samples: æ¯ä¸ªæ¡ä»¶ä¸‹çš„æœ€å¤§æ ·æœ¬æ•°ï¼Œé»˜è®¤50å‡å°‘å†…å­˜ä½¿ç”¨

    Returns:
        results_dict: æŒ‰æ•°æ®é›†ã€æ¨¡å‹è§„æ¨¡ã€æ¨¡å‹ç±»å‹ã€å‰ç¼€é•¿åº¦å’Œç”Ÿæˆé•¿åº¦ç»„ç»‡çš„ç»“æœ
    """
    results_dict = {}

    # é»˜è®¤å‚æ•°è®¾ç½® - å†…å­˜ä¼˜åŒ–
    if datasets is None:
        datasets = ['stackexchange', 'dclm-privacy', 'wiki-fact']
    if prefix_lengths is None:
        prefix_lengths = [16, 32, 64]
    if generation_lengths is None:
        generation_lengths = [8, 16, 128]
    if max_samples is None:
        max_samples = 50  # å†…å­˜ä¼˜åŒ–ï¼šé»˜è®¤é™åˆ¶æ ·æœ¬æ•°

    print(f"å†…å­˜ä¼˜åŒ–æ¨¡å¼: æ¯ä¸ªé…ç½®æœ€å¤šåŠ è½½ {max_samples} ä¸ªæ ·æœ¬")
    print(f"å¼€å§‹ä»åŸºç¡€ç›®å½•åŠ è½½æ•°æ®: {results_base_dir}")

    # éå†æ¯ä¸ªgeneration_lengthå¯¹åº”çš„æ–‡ä»¶å¤¹
    for gen_length in generation_lengths:
        gen_folder = f"exp1_generation_{gen_length}"
        gen_dir = os.path.join(results_base_dir, gen_folder)

        if not os.path.exists(gen_dir):
            print(f"è­¦å‘Š: æ–‡ä»¶å¤¹ {gen_dir} ä¸å­˜åœ¨ï¼Œè·³è¿‡generation_length={gen_length}")
            continue

        print(f"\nå¤„ç†generation_length={gen_length}çš„æ–‡ä»¶å¤¹: {gen_folder}")

        # æœç´¢è¯¥æ–‡ä»¶å¤¹å†…çš„æ‰€æœ‰jsonlæ–‡ä»¶
        pattern = os.path.join(gen_dir, "*.jsonl")
        result_files = glob.glob(pattern)

        if not result_files:
            print(f"è­¦å‘Š: åœ¨ {gen_dir} ä¸­æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶")
            continue

        print(f"åœ¨ {gen_folder} ä¸­æ‰¾åˆ° {len(result_files)} ä¸ªç»“æœæ–‡ä»¶")

        for filepath in result_files:
            try:
                # ä»æ–‡ä»¶åè§£æä¿¡æ¯
                filename = os.path.basename(filepath)
                print(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {filename}")

                filename_parts = filename.replace('.jsonl', '').split('_')

                if len(filename_parts) >= 5:
                    dataset = filename_parts[0]
                    prefix_info = filename_parts[1]
                    file_model_scale = filename_parts[2]
                    model_type = filename_parts[3]

                    # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                    if datasets and dataset not in datasets:
                        continue
                    if model_scales and file_model_scale not in model_scales:
                        continue

                    try:
                        prefix_length = int(prefix_info.replace('prefix', ''))
                    except:
                        continue

                    if prefix_lengths and prefix_length not in prefix_lengths:
                        continue

                    # å†…å­˜ä¼˜åŒ–ï¼šåˆ†æ‰¹è¯»å–æ–‡ä»¶
                    samples = []
                    sample_count = 0

                    with open(filepath, 'r', encoding='utf-8') as f:
                        for line_idx, line in enumerate(f):
                            if line.strip() and sample_count < max_samples:
                                try:
                                    sample = json.loads(line)
                                    samples.append(sample)
                                    sample_count += 1

                                except json.JSONDecodeError:
                                    continue

                    if not samples:
                        continue

                    # ç»„ç»‡æ•°æ®ç»“æ„
                    if dataset not in results_dict:
                        results_dict[dataset] = {}
                    if file_model_scale not in results_dict[dataset]:
                        results_dict[dataset][file_model_scale] = {'base': {}, 'sft': {}}
                    if prefix_length not in results_dict[dataset][file_model_scale][model_type]:
                        results_dict[dataset][file_model_scale][model_type][prefix_length] = {}
                    if gen_length not in results_dict[dataset][file_model_scale][model_type][prefix_length]:
                        results_dict[dataset][file_model_scale][model_type][prefix_length][gen_length] = []

                    results_dict[dataset][file_model_scale][model_type][prefix_length][gen_length].extend(samples)
                    print(
                        f"âœ“ åŠ è½½ {dataset}-{file_model_scale}-{model_type}-prefix{prefix_length}-gen{gen_length}: {len(samples)} æ¡æ ·æœ¬")

            except Exception as e:
                print(f"åŠ è½½æ–‡ä»¶ {filepath} æ—¶å‡ºé”™: {e}")
                continue

    print(f"\nå†…å­˜ä¼˜åŒ–åŠ è½½å®Œæˆ!")
    return results_dict
    """
    åŠ è½½å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹è§„æ¨¡çš„ç”Ÿæˆç»“æœæ–‡ä»¶
    æ”¯æŒæŒ‰generation_lengthåˆ†æ–‡ä»¶å¤¹å­˜å‚¨çš„ç»“æ„

    Args:
        results_base_dir: ç»“æœåŸºç¡€ç›®å½•è·¯å¾„ï¼ŒåŒ…å«exp1_generation_Xå­æ–‡ä»¶å¤¹
        model_scales: æ¨¡å‹è§„æ¨¡åˆ—è¡¨ (å¦‚ ["1B", "7B", "13B", "32B"])
        datasets: è¦åŠ è½½çš„æ•°æ®é›†åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºåŠ è½½æ‰€æœ‰
        prefix_lengths: è¦åŠ è½½çš„å‰ç¼€é•¿åº¦åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºåŠ è½½æ‰€æœ‰
        generation_lengths: è¦åŠ è½½çš„ç”Ÿæˆé•¿åº¦åˆ—è¡¨(continuation L)ï¼ŒNoneè¡¨ç¤ºåŠ è½½æ‰€æœ‰
        max_samples: æ¯ä¸ªæ¡ä»¶ä¸‹çš„æœ€å¤§æ ·æœ¬æ•°ï¼ŒNoneè¡¨ç¤ºåŠ è½½æ‰€æœ‰

    Returns:
        results_dict: æŒ‰æ•°æ®é›†ã€æ¨¡å‹è§„æ¨¡ã€æ¨¡å‹ç±»å‹ã€å‰ç¼€é•¿åº¦å’Œç”Ÿæˆé•¿åº¦ç»„ç»‡çš„ç»“æœ
        æ ¼å¼: {dataset: {model_scale: {model_type: {prefix_length: {generation_length: [samples]}}}}}
    """
    results_dict = {}

    # é»˜è®¤å‚æ•°è®¾ç½® - åŸºäºç”¨æˆ·éœ€æ±‚
    if datasets is None:
        datasets = ['stackexchange', 'dclm-privacy', 'wiki-fact']
    if prefix_lengths is None:
        prefix_lengths = [16, 32, 64]
    if generation_lengths is None:
        generation_lengths = [8, 16, 128]

    print(f"å¼€å§‹ä»åŸºç¡€ç›®å½•åŠ è½½æ•°æ®: {results_base_dir}")
    print(f"ç›®æ ‡generation lengths: {generation_lengths}")

    # éå†æ¯ä¸ªgeneration_lengthå¯¹åº”çš„æ–‡ä»¶å¤¹
    for gen_length in generation_lengths:
        gen_folder = f"exp1_generation_{gen_length}"
        gen_dir = os.path.join(results_base_dir, gen_folder)

        if not os.path.exists(gen_dir):
            print(f"è­¦å‘Š: æ–‡ä»¶å¤¹ {gen_dir} ä¸å­˜åœ¨ï¼Œè·³è¿‡generation_length={gen_length}")
            continue

        print(f"\nå¤„ç†generation_length={gen_length}çš„æ–‡ä»¶å¤¹: {gen_folder}")

        # æœç´¢è¯¥æ–‡ä»¶å¤¹å†…çš„æ‰€æœ‰jsonlæ–‡ä»¶
        pattern = os.path.join(gen_dir, "*.jsonl")
        result_files = glob.glob(pattern)

        if not result_files:
            print(f"è­¦å‘Š: åœ¨ {gen_dir} ä¸­æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶")
            continue

        print(f"åœ¨ {gen_folder} ä¸­æ‰¾åˆ° {len(result_files)} ä¸ªç»“æœæ–‡ä»¶")

        for filepath in result_files:
            try:
                # ä»æ–‡ä»¶åè§£æä¿¡æ¯
                filename = os.path.basename(filepath)
                print(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {filename}")

                # ç§»é™¤.jsonlåç¼€
                filename_parts = filename.replace('.jsonl', '').split('_')

                # è§£ææ–‡ä»¶åæ ¼å¼: dataset_prefix{length}_{model_scale}_{model_type}_{num_samples}_samples.jsonl
                if len(filename_parts) >= 5:
                    dataset = filename_parts[0]
                    prefix_info = filename_parts[1]  # prefix{length}
                    file_model_scale = filename_parts[2]
                    model_type = filename_parts[3]

                    # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                    if datasets and dataset not in datasets:
                        print(f"è·³è¿‡æ•°æ®é›† {dataset} (ä¸åœ¨ç›®æ ‡åˆ—è¡¨ä¸­)")
                        continue

                    if model_scales and file_model_scale not in model_scales:
                        print(f"è·³è¿‡æ¨¡å‹è§„æ¨¡ {file_model_scale} (ä¸åœ¨ç›®æ ‡åˆ—è¡¨ä¸­)")
                        continue

                    # æå–å‰ç¼€é•¿åº¦
                    try:
                        prefix_length = int(prefix_info.replace('prefix', ''))
                    except:
                        print(f"æ— æ³•è§£æå‰ç¼€é•¿åº¦: {prefix_info}")
                        continue

                    if prefix_lengths and prefix_length not in prefix_lengths:
                        print(f"è·³è¿‡å‰ç¼€é•¿åº¦ {prefix_length} (ä¸åœ¨ç›®æ ‡åˆ—è¡¨ä¸­)")
                        continue

                    # åŠ è½½jsonlæ•°æ®
                    samples = []

                    with open(filepath, 'r', encoding='utf-8') as f:
                        for line_idx, line in enumerate(f):
                            if line.strip():
                                try:
                                    sample = json.loads(line)
                                    samples.append(sample)

                                except json.JSONDecodeError as e:
                                    print(f"JSONè§£æé”™è¯¯ åœ¨æ–‡ä»¶ {filename} ç¬¬ {line_idx + 1} è¡Œ: {e}")
                                    continue

                    if not samples:
                        print(f"æ–‡ä»¶ {filename} ä¸­æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬")
                        continue

                    # åº”ç”¨æ ·æœ¬æ•°è¿‡æ»¤
                    if max_samples and len(samples) > max_samples:
                        samples = samples[:max_samples]
                        print(f"æ ·æœ¬æ•°é‡é™åˆ¶ä¸º {max_samples}")

                    # ç»„ç»‡æ•°æ®ç»“æ„
                    if dataset not in results_dict:
                        results_dict[dataset] = {}
                    if file_model_scale not in results_dict[dataset]:
                        results_dict[dataset][file_model_scale] = {'base': {}, 'sft': {}}
                    if prefix_length not in results_dict[dataset][file_model_scale][model_type]:
                        results_dict[dataset][file_model_scale][model_type][prefix_length] = {}
                    if gen_length not in results_dict[dataset][file_model_scale][model_type][prefix_length]:
                        results_dict[dataset][file_model_scale][model_type][prefix_length][gen_length] = []

                    results_dict[dataset][file_model_scale][model_type][prefix_length][gen_length].extend(samples)
                    print(
                        f"âœ“ åŠ è½½ {dataset}-{file_model_scale}-{model_type}-prefix{prefix_length}-gen{gen_length}: {len(samples)} æ¡æ ·æœ¬")

                else:
                    print(f"è­¦å‘Š: æ–‡ä»¶åæ ¼å¼ä¸ç¬¦åˆé¢„æœŸ: {filename}")

            except Exception as e:
                print(f"åŠ è½½æ–‡ä»¶ {filepath} æ—¶å‡ºé”™: {e}")
                continue

    # æ‰“å°åŠ è½½æ‘˜è¦
    total_configs = 0
    for dataset in results_dict:
        for model_scale in results_dict[dataset]:
            for model_type in ['base', 'sft']:
                if model_type in results_dict[dataset][model_scale]:
                    for prefix_length in results_dict[dataset][model_scale][model_type]:
                        total_configs += len(results_dict[dataset][model_scale][model_type][prefix_length])

    print(f"\næ•°æ®åŠ è½½å®Œæˆ! æ€»å…±åŠ è½½äº† {total_configs} ä¸ªé…ç½®ç»„åˆ")
    return results_dict


def calculate_memorization_metrics_with_evaluator(results_dict: Dict[str, Dict]) -> pd.DataFrame:
    """
    ä½¿ç”¨MemorizationMetricsç±»è®¡ç®—è®°å¿†æŒ‡æ ‡ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰

    Args:
        results_dict: ç”Ÿæˆç»“æœå­—å…¸

    Returns:
        metrics_df: è®°å¿†æŒ‡æ ‡çš„DataFrame
    """
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    try:
        evaluator = MemorizationMetrics()
        print("âœ“ MemorizationMetricsè¯„ä¼°å™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ è¯„ä¼°å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return pd.DataFrame()

    metrics_data = []

    for dataset in results_dict:
        for model_scale in results_dict[dataset]:
            for model_type in ['base', 'sft']:
                if model_type not in results_dict[dataset][model_scale]:
                    continue

                for prefix_length in results_dict[dataset][model_scale][model_type]:
                    for generation_length, samples in results_dict[dataset][model_scale][model_type][
                        prefix_length].items():
                        if not samples:
                            continue

                        print(
                            f"è®¡ç®— {dataset}-{model_scale}-{model_type}-prefix{prefix_length}-gen{generation_length} çš„è®°å¿†æŒ‡æ ‡...")

                        try:
                            # ä½¿ç”¨æ­£ç¡®çš„è¯„ä¼°å™¨æ¥å£
                            metrics_results = evaluator.compute_all_metrics_from_data(samples)

                            # æå–å„ç§æŒ‡æ ‡æ•°æ®
                            metrics_entry = {
                                'dataset': dataset,
                                'model_type': model_type,
                                'model_scale': model_scale,
                                'prefix_length': prefix_length,
                                'generation_length': generation_length,
                                'sample_count': len(samples),
                            }

                            # ç¬¬ä¸€ç§ï¼šç²¾ç¡®åŒ¹é…
                            if 'exact_match' in metrics_results:
                                metrics_entry['exact_match_rate'] = metrics_results['exact_match']['exact_match_rate']

                            # ç¬¬äºŒç§ï¼šROUGE/BLEUæŒ‡æ ‡
                            if 'rouge_bleu' in metrics_results:
                                rouge_bleu = metrics_results['rouge_bleu']
                                metrics_entry['rouge_1_f'] = rouge_bleu.get('rouge_1_f', 0.0)
                                metrics_entry['rouge_2_f'] = rouge_bleu.get('rouge_2_f', 0.0)
                                metrics_entry['rouge_l_f'] = rouge_bleu.get('rouge_l_f', 0.0)
                                metrics_entry['bleu_1'] = rouge_bleu.get('bleu_1', 0.0)
                                metrics_entry['bleu_2'] = rouge_bleu.get('bleu_2', 0.0)
                                metrics_entry['bleu_4'] = rouge_bleu.get('bleu_4', 0.0)

                            # ç¬¬ä¸‰ç§ï¼šç¼–è¾‘è·ç¦»
                            if 'edit_distance' in metrics_results:
                                edit_dist = metrics_results['edit_distance']
                                metrics_entry['token_edit_distance'] = edit_dist.get('token_edit_distance', 0.0)

                            # ç¬¬äº”ç§ï¼šæ¦‚ç‡ç›¸å…³æŒ‡æ ‡
                            if 'likelihood' in metrics_results:
                                likelihood = metrics_results['likelihood']
                                metrics_entry['target_token_probability'] = likelihood.get('target_token_probability',
                                                                                           0.0)
                                metrics_entry['target_token_rank'] = likelihood.get('target_token_rank', float('inf'))
                                metrics_entry['target_in_top1_rate'] = likelihood.get('target_in_top1_rate', 0.0)
                                metrics_entry['target_in_top5_rate'] = likelihood.get('target_in_top5_rate', 0.0)
                            else:
                                # å¦‚æœæ²¡æœ‰likelihoodæ•°æ®ï¼Œè®¾ç½®é»˜è®¤å€¼
                                metrics_entry['target_token_probability'] = 0.0
                                metrics_entry['target_token_rank'] = float('inf')
                                metrics_entry['target_in_top1_rate'] = 0.0
                                metrics_entry['target_in_top5_rate'] = 0.0

                            metrics_data.append(metrics_entry)
                            print(f"âœ“ å®Œæˆè®¡ç®—ï¼Œå¤„ç† {len(samples)} ä¸ªæ ·æœ¬")

                        except Exception as e:
                            print(f"âŒ è®¡ç®—æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
                            continue

                        # å†…å­˜æ¸…ç†
                        del samples

    if not metrics_data:
        print("âŒ æ²¡æœ‰æˆåŠŸè®¡ç®—ä»»ä½•æŒ‡æ ‡")
        return pd.DataFrame()

    return pd.DataFrame(metrics_data)


def generate_latex_tables(metrics_df: pd.DataFrame,
                          output_dir: str,
                          prefix_lengths: List[int],
                          generation_lengths: List[int]):
    """
    ç”ŸæˆLaTeXè¡¨æ ¼ï¼Œæ¯ä¸ªè¯„ä¼°æŒ‡æ ‡ç”Ÿæˆä¸€ä¸ªè¡¨æ ¼
    æŒ‰ç…§ç”¨æˆ·éœ€æ±‚çš„æ ¼å¼ç”Ÿæˆè¡¨æ ¼

    Args:
        metrics_df: åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„DataFrame
        output_dir: è¾“å‡ºç›®å½•
        prefix_lengths: å‰ç¼€é•¿åº¦åˆ—è¡¨
        generation_lengths: ç”Ÿæˆé•¿åº¦åˆ—è¡¨
    """

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)

    # æ•°æ®é›†åç§°æ˜ å°„ï¼ˆä¸­è‹±æ–‡å¯¹ç…§ï¼‰
    dataset_mapping = {
        'stackexchange': 'STACKEXCHANGE',
        'dclm-privacy': 'DCLM-PRIVACY',
        'wiki-fact': 'WIKI-FACT'
    }

    # éœ€è¦ç”Ÿæˆçš„æŒ‡æ ‡åŠå…¶æ˜¾ç¤ºåç§°ï¼ˆåŸºäºMemorizationMetricsçš„è¾“å‡ºï¼‰
    metrics_to_generate = [
        ('exact_match_rate', 'Exact Match Rate'),
        ('rouge_1_f', 'ROUGE-1 F-score'),
        ('rouge_2_f', 'ROUGE-2 F-score'),
        ('rouge_l_f', 'ROUGE-L F-score'),
        ('bleu_1', 'BLEU-1'),
        ('bleu_2', 'BLEU-2'),
        ('bleu_4', 'BLEU-4'),
        ('token_edit_distance', 'Token Edit Distance'),
        ('target_token_probability', 'Target Token Probability')
    ]

    # æ”¶é›†æ‰€æœ‰latexè¡¨æ ¼
    all_latex_tables = []

    # è·å–å”¯ä¸€çš„æ•°æ®é›†å’Œæ¨¡å‹
    datasets = sorted(metrics_df['dataset'].unique())
    model_scales = sort_model_scales(metrics_df['model_scale'].unique())

    print(f"å‘ç°æ•°æ®é›†: {datasets}")
    print(f"å‘ç°æ¨¡å‹è§„æ¨¡: {model_scales}")

    # ä¸ºæ¯ä¸ªprefix_lengthå’Œgeneration_lengthç»„åˆç”Ÿæˆè¡¨æ ¼
    for prefix_length in prefix_lengths:
        for generation_length in generation_lengths:
            print(f"\n=== ç”Ÿæˆ prefix_length={prefix_length}, generation_length={generation_length} çš„è¡¨æ ¼ ===")

            # ç­›é€‰å½“å‰æ¡ä»¶çš„æ•°æ®
            condition_df = metrics_df[
                (metrics_df['prefix_length'] == prefix_length) &
                (metrics_df['generation_length'] == generation_length)
                ]

            if len(condition_df) == 0:
                print(f"è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ° prefix_length={prefix_length}, generation_length={generation_length} çš„æ•°æ®")
                continue

            print(f"æ‰¾åˆ° {len(condition_df)} æ¡è®°å½•")

            # ä¸ºæ¯ä¸ªæŒ‡æ ‡ç”Ÿæˆè¡¨æ ¼
            for metric_col, metric_name in metrics_to_generate:
                if metric_col not in condition_df.columns:
                    print(f"è­¦å‘Š: æŒ‡æ ‡ {metric_col} ä¸åœ¨æ•°æ®ä¸­")
                    continue

                print(f"ç”Ÿæˆ {metric_name} çš„è¡¨æ ¼ (prefix={prefix_length}, gen={generation_length})...")

                # åˆ›å»ºè¡¨æ ¼æ•°æ®
                table_data = []
                row_labels = []

                for model_scale in model_scales:
                    for model_type in ['base', 'sft']:
                        # åˆ›å»ºè¡Œæ ‡ç­¾ï¼šæ¨¡å‹è§„æ¨¡ + æ¨¡å‹ç±»å‹
                        if model_type == 'base':
                            model_label = f"{model_scale}"
                        else:
                            model_label = f"{model_scale}"  # SFTè¡Œ
                        row_labels.append(model_label)

                        row_data = []
                        for dataset in datasets:
                            # æŸ¥æ‰¾å¯¹åº”çš„å€¼
                            mask = (condition_df['dataset'] == dataset) & \
                                   (condition_df['model_scale'] == model_scale) & \
                                   (condition_df['model_type'] == model_type)

                            if mask.sum() > 0:
                                value = condition_df.loc[mask, metric_col].iloc[0]
                                if pd.isna(value) or value == float('inf') or value == float('-inf'):
                                    row_data.append('N/A')
                                else:
                                    # æ ¹æ®æŒ‡æ ‡ç±»å‹å†³å®šæ ¼å¼
                                    if metric_col == 'token_edit_distance':
                                        row_data.append(f"{value:.1f}")
                                    else:
                                        row_data.append(f"{value:.3f}")
                            else:
                                row_data.append('N/A')

                        table_data.append(row_data)

                # ç”Ÿæˆlatexè¡¨æ ¼ï¼Œæ ‡é¢˜åŒ…å«prefix_lengthå’Œgeneration_lengthä¿¡æ¯
                table_title = f"{metric_name} (Prefix L: {prefix_length}, Generation L: {generation_length})"
                latex_table = generate_single_latex_table(
                    table_data,
                    row_labels,
                    [dataset_mapping.get(d, d) for d in datasets],
                    table_title,
                    model_scales  # ä¼ å…¥æ¨¡å‹è§„æ¨¡ç”¨äºåˆ†ç»„
                )

                all_latex_tables.append(latex_table)
                print(f"âœ“ {metric_name} è¡¨æ ¼å·²ç”Ÿæˆ")

    # ä¿å­˜æ‰€æœ‰è¡¨æ ¼åˆ°ä¸€ä¸ªæ–‡ä»¶
    prefix_str = '_'.join(map(str, prefix_lengths))
    gen_str = '_'.join(map(str, generation_lengths))
    output_file = os.path.join(output_dir, f'memorization_metrics_latex_prefix{prefix_str}_gen{gen_str}.tex')

    with open(output_file, 'w', encoding='utf-8') as f:
        # å†™å…¥LaTeXæ–‡æ¡£å¤´éƒ¨
        f.write("% Memorization Metrics LaTeX Tables\n")
        f.write("% Generated automatically\n")
        f.write(f"% Prefix lengths: {prefix_lengths}\n")
        f.write(f"% Generation lengths: {generation_lengths}\n")
        f.write("% Requires booktabs package: \\usepackage{booktabs}\n\n")

        for i, table in enumerate(all_latex_tables):
            f.write(table)
            if i < len(all_latex_tables) - 1:
                f.write("\n\n\\clearpage\n\n")

    print(f"\næ‰€æœ‰latexè¡¨æ ¼å·²ä¿å­˜åˆ°: {output_file}")
    print(f"æ€»å…±ç”Ÿæˆäº† {len(all_latex_tables)} ä¸ªè¡¨æ ¼")

    # ä¹Ÿç”Ÿæˆä¸€ä¸ªæ€»ç»“çš„CSVæ–‡ä»¶
    summary_file = os.path.join(output_dir, f'memorization_metrics_summary_prefix{prefix_str}_gen{gen_str}.csv')
    metrics_df.to_csv(summary_file, index=False, encoding='utf-8')
    print(f"æŒ‡æ ‡æ€»ç»“CSVå·²ä¿å­˜åˆ°: {summary_file}")

    # æ‰“å°è¡¨æ ¼å†…å®¹åˆ°æ§åˆ¶å°
    print("\n=== è¡¨æ ¼å†…å®¹é¢„è§ˆ ===")
    for i, table in enumerate(all_latex_tables[:2]):  # åªæ˜¾ç¤ºå‰ä¸¤ä¸ªè¡¨æ ¼é¿å…è¾“å‡ºè¿‡é•¿
        print(f"\nè¡¨æ ¼ {i + 1}:")
        print(table)
        if i >= 1:
            print(f"\n... (å…± {len(all_latex_tables)} ä¸ªè¡¨æ ¼ï¼Œå®Œæ•´å†…å®¹è¯·æŸ¥çœ‹æ–‡ä»¶)")
            break


def generate_single_latex_table(table_data: List[List[str]],
                                row_labels: List[str],
                                col_labels: List[str],
                                table_title: str,
                                model_scales: List[str] = None) -> str:
    """
    ç”Ÿæˆå•ä¸ªlatexè¡¨æ ¼ï¼Œæ”¯æŒæ¨¡å‹åˆ†ç»„æ˜¾ç¤º

    Args:
        table_data: è¡¨æ ¼æ•°æ®
        row_labels: è¡Œæ ‡ç­¾
        col_labels: åˆ—æ ‡ç­¾
        table_title: è¡¨æ ¼æ ‡é¢˜
        model_scales: æ¨¡å‹è§„æ¨¡åˆ—è¡¨ï¼Œç”¨äºæ·»åŠ åˆ†ç»„çº¿

    Returns:
        latex_code: ç”Ÿæˆçš„latexä»£ç 
    """
    num_cols = len(col_labels)

    # å¼€å§‹è¡¨æ ¼
    latex_lines = [
        "\\begin{table}[h]",
        "\\centering",
        f"\\caption{{{table_title}}}",
        f"\\begin{{tabular}}{{l{'c' * num_cols}}}",
        "\\toprule"
    ]

    # è¡¨å¤´
    header = "Model & " + " & ".join(col_labels) + " \\\\"
    latex_lines.append(header)
    latex_lines.append("\\midrule")

    # è¡¨æ ¼æ•°æ®ï¼Œæ¯ä¸¤è¡Œï¼ˆbaseå’Œsftï¼‰ä¸ºä¸€ç»„
    for i, (row_label, row_data) in enumerate(zip(row_labels, table_data)):
        # å¦‚æœæ˜¯SFTè¡Œï¼Œåœ¨è¡Œæ ‡ç­¾å‰æ·»åŠ ç¼©è¿›
        if i % 2 == 1:  # SFTè¡Œ
            row_str = "~~~" + row_label + " & " + " & ".join(row_data) + " \\\\"
        else:  # Baseè¡Œ
            row_str = row_label + " & " + " & ".join(row_data) + " \\\\"

        latex_lines.append(row_str)

        # æ¯ä¸ªæ¨¡å‹çš„baseå’Œsftä¹‹é—´æ·»åŠ å°åˆ†éš”
        if i % 2 == 1 and i < len(row_labels) - 1:
            latex_lines.append("\\addlinespace[0.1em]")

    # ç»“æŸè¡¨æ ¼
    latex_lines.extend([
        "\\bottomrule",
        "\\end{tabular}",
        f"\\label{{tab:{table_title.lower().replace(' ', '_').replace('-', '_').replace(':', '').replace('(', '').replace(')', '')}}}",
        "\\end{table}"
    ])

    return "\n".join(latex_lines)


def main():
    """ä¸»å‡½æ•°ï¼Œè§£æå‚æ•°å¹¶æ‰§è¡Œè¡¨æ ¼ç”Ÿæˆæµç¨‹"""

    parser = argparse.ArgumentParser(description='ç”Ÿæˆè®°å¿†æŒ‡æ ‡çš„LaTeXè¡¨æ ¼')

    parser.add_argument('--results_base_dir', type=str,
                        default='/root/autodl-tmp/ift_memorization/results',
                        help='ç»“æœåŸºç¡€ç›®å½•è·¯å¾„ï¼ŒåŒ…å«exp1_generation_Xå­æ–‡ä»¶å¤¹')
    parser.add_argument('--model_scales', type=str, nargs='+',
                        default=['1B', '7B', '13B', '32B'],
                        help='è¦åˆ†æçš„æ¨¡å‹è§„æ¨¡åˆ—è¡¨ï¼Œå¦‚ ["1B", "7B", "13B", "32B"]')
    parser.add_argument('--datasets', type=str, nargs='+',
                        default=['stackexchange', 'dclm-privacy', 'wiki-fact'],
                        help='è¦åˆ†æçš„æ•°æ®é›†åˆ—è¡¨')
    parser.add_argument('--prefix_lengths', type=int, nargs='+',
                        default=[16, 32, 64],
                        help='è¦åˆ†æçš„å‰ç¼€é•¿åº¦åˆ—è¡¨')
    parser.add_argument('--generation_lengths', type=int, nargs='+',
                        default=[8, 16, 128],
                        help='è¦åˆ†æçš„ç”Ÿæˆé•¿åº¦åˆ—è¡¨(continuation L)')
    parser.add_argument('--max_samples', type=int, default=100,
                        help='æ¯ç§æ¡ä»¶ä¸‹çš„æœ€å¤§æ ·æœ¬æ•°ï¼Œè®¾ç½®è¾ƒå°å€¼ä»¥é€‚åº”2Gå†…å­˜é™åˆ¶')
    parser.add_argument('--output_dir', type=str,
                        default='/root/autodl-tmp/ift_memorization/results/latex_tables',
                        help='LaTeXè¡¨æ ¼è¾“å‡ºç›®å½•')

    args = parser.parse_args()

    print("=" * 80)
    print("å¼€å§‹ç”Ÿæˆè®°å¿†æŒ‡æ ‡LaTeXè¡¨æ ¼...")
    print("=" * 80)
    print(f"ç»“æœåŸºç¡€ç›®å½•: {args.results_base_dir}")
    print(f"æ¨¡å‹è§„æ¨¡: {args.model_scales}")
    print(f"æ•°æ®é›†: {args.datasets}")
    print(f"å‰ç¼€é•¿åº¦: {args.prefix_lengths}")
    print(f"ç”Ÿæˆé•¿åº¦: {args.generation_lengths}")
    print(f"æœ€å¤§æ ·æœ¬æ•°: {args.max_samples}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print("=" * 80)

    # åŠ è½½ç”Ÿæˆç»“æœ
    print("\næ­¥éª¤1: åŠ è½½ç”Ÿæˆç»“æœ...")
    results_dict = load_generation_results_memory_optimized(
        args.results_base_dir,
        args.model_scales,
        args.datasets,
        args.prefix_lengths,
        args.generation_lengths,
        args.max_samples
    )

    if not results_dict:
        print("âŒ é”™è¯¯: æœªèƒ½åŠ è½½ä»»ä½•ç”Ÿæˆç»“æœ")
        return

    # è®¡ç®—è®°å¿†æŒ‡æ ‡
    print("\næ­¥éª¤2: è®¡ç®—è®°å¿†æŒ‡æ ‡...")
    metrics_df = calculate_memorization_metrics_with_evaluator(results_dict)

    if len(metrics_df) == 0:
        print("âŒ é”™è¯¯: æ— æ³•è®¡ç®—è®°å¿†æŒ‡æ ‡")
        return

    print(f"âœ“ è®¡ç®—å®Œæˆï¼Œå…± {len(metrics_df)} æ¡è®°å½•")
    print("\næŒ‡æ ‡æ¦‚è§ˆ:")
    if len(metrics_df) > 0:
        preview_cols = ['dataset', 'model_scale', 'model_type', 'prefix_length',
                        'generation_length', 'exact_match_rate', 'rouge_1_f', 'bleu_1']
        available_cols = [col for col in preview_cols if col in metrics_df.columns]
        print(metrics_df[available_cols].head(10))

    # ç”Ÿæˆå¹¶ä¿å­˜latexè¡¨æ ¼
    print("\næ­¥éª¤3: ç”ŸæˆLaTeXè¡¨æ ¼...")
    generate_latex_tables(metrics_df, args.output_dir, args.prefix_lengths, args.generation_lengths)

    print("\n" + "=" * 80)
    print("ğŸ‰ åˆ†æå®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    main()