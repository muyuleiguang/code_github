#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®éªŒäºŒï¼šåˆ†æè®°å¿†åŒ–å˜åŒ–ä¸ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½å˜åŒ–ä¹‹é—´çš„å…³ç³»
Author: Research Team
Date: 2025

ä¸»è¦åŠŸèƒ½ï¼š
1. åŠ è½½è®°å¿†åŒ–è¯„ä¼°ç»“æœï¼ˆbase vs SFTï¼‰
2. åŠ è½½ä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°ç»“æœï¼ˆbase vs SFTï¼‰
3. è®¡ç®—å˜åŒ–é‡ï¼ˆchange_1: memorizationå˜åŒ–ï¼Œchange_2: downstreamå˜åŒ–ï¼‰
4. è¿›è¡Œç›¸å…³æ€§åˆ†æå’Œå› æœåˆ†æ
5. ä¿å­˜ç»“æ„åŒ–ç»“æœå¹¶å¯è§†åŒ–å±•ç¤º
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import argparse
import json
from scipy import stats
from scipy.stats import spearmanr, pearsonr
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class MemorizationDownstreamAnalyzer:
    """è®°å¿†åŒ–ä¸ä¸‹æ¸¸ä»»åŠ¡å…³ç³»åˆ†æå™¨"""

    def __init__(self, memorization_dir, output_dir, save_prefix="exp2_analysis"):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            memorization_dir (str): è®°å¿†åŒ–ç»“æœç›®å½•è·¯å¾„
            output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
            save_prefix (str): ä¿å­˜æ–‡ä»¶çš„å‰ç¼€
        """
        self.memorization_dir = memorization_dir
        self.output_dir = output_dir
        self.save_prefix = save_prefix

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # æ¨¡å‹è§„æ¨¡å’Œæ•°æ®é›†ç±»å‹
        self.scales = ['1B', '7B']
        self.datasets = ['stackexchange', 'wiki-fact', 'dclm-privacy']  # dclmå¯¹åº”å®‰å…¨æ•æ„Ÿå†…å®¹

        # è®°å¿†åŒ–æŒ‡æ ‡ï¼ˆé€‰æ‹©æœ€é‡è¦çš„å‡ ä¸ªï¼‰
        self.mem_metrics = [
            'exact_match_rate',  # ç²¾ç¡®åŒ¹é…ç‡
            'rouge_2_f',  # ROUGE-L F1åˆ†æ•°
            'bleu_2',  # BLEU-4åˆ†æ•°
            # 'semantic_similarity',  # è¯­ä¹‰ç›¸ä¼¼åº¦
            'target_token_probability',  # ç›®æ ‡tokenæ¦‚ç‡
            'target_in_top1_rate'  # ç›®æ ‡tokenåœ¨top-1ä¸­çš„å‘½ä¸­ç‡
        ]

        # ä¸‹æ¸¸ä»»åŠ¡ï¼ˆæ ¹æ®ç”¨æˆ·æä¾›çš„LaTeXè¡¨æ ¼ï¼‰
        self.downstream_tasks = ['GSM8K', 'MATH', 'MMLU', 'PopQA']

        # åˆå§‹åŒ–ç»“æœå­˜å‚¨
        self.results = {}

    def load_memorization_results(self):
        """
        åŠ è½½è®°å¿†åŒ–è¯„ä¼°ç»“æœ

        Returns:
            pd.DataFrame: åŒ…å«æ‰€æœ‰è®°å¿†åŒ–ç»“æœçš„æ•°æ®æ¡†
        """
        print("ğŸ“Š å¼€å§‹åŠ è½½è®°å¿†åŒ–è¯„ä¼°ç»“æœ...")

        # ä»é™„ä»¶CSVæ–‡ä»¶ä¸­è¯»å–æ•°æ®
        filepath = "/root/autodl-tmp/ift_memorization/results/latex_tables/memorization_metrics_summary_prefix16_32_64_gen8_16_128.csv"

        if os.path.exists(filepath):
            print(f"   âœ… åŠ è½½: {os.path.basename(filepath)}")
            mem_df = pd.read_csv(filepath)
            print(f"   ğŸ“ˆ æ€»è®¡åŠ è½½ {len(mem_df)} æ¡è®°å¿†åŒ–ç»“æœè®°å½•")
            return mem_df
        else:
            raise FileNotFoundError("æ²¡æœ‰æ‰¾åˆ°è®°å¿†åŒ–ç»“æœæ–‡ä»¶ï¼")

    def create_downstream_results(self):
        """
        åˆ›å»ºä¸‹æ¸¸ä»»åŠ¡ç»“æœæ•°æ®æ¡†
        åŸºäºç”¨æˆ·æä¾›çš„LaTeXè¡¨æ ¼æ•°æ®

        Returns:
            pd.DataFrame: åŒ…å«æ‰€æœ‰ä¸‹æ¸¸ä»»åŠ¡ç»“æœçš„æ•°æ®æ¡†
        """
        print("ğŸ“Š åˆ›å»ºä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°ç»“æœ...")

        # æ ¹æ®ç”¨æˆ·æä¾›çš„LaTeXè¡¨æ ¼æ•°æ®
        # æ³¨æ„ï¼šç”¨æˆ·æä¾›çš„æ•°æ®ä¸­æ‰€æœ‰å€¼éƒ½ç›¸åŒï¼Œè¿™å¯èƒ½æ˜¯ç¤ºä¾‹æ•°æ®
        # åœ¨å®é™…æƒ…å†µä¸‹ï¼Œè¿™äº›å€¼åº”è¯¥æ˜¯ä¸åŒçš„
        downstream_data = []

        for i, scale in enumerate(self.scales):
            for model_type in ['base', 'sft']:
                model_name = f"OLMo-2-{scale}" if model_type == 'base' else f"OLMo-2-{scale}-SFT"
                # ä»»åŠ¡å¯¹åº”çš„åˆ†æ•°ï¼ˆä»LaTeXè¡¨æ ¼ä¸­æå–ï¼‰
                task_scores = TASK_SCORES[model_name]

                record = {
                    'model_name': model_name,
                    'model_type': model_type,
                    'scale': scale
                }

                # æ·»åŠ å„ä»»åŠ¡åˆ†æ•°
                for j, task in enumerate(self.downstream_tasks):
                    record[task] = task_scores[j]

                downstream_data.append(record)

        downstream_df = pd.DataFrame(downstream_data)
        print(f"   ğŸ“ˆ åˆ›å»ºäº† {len(downstream_df)} æ¡ä¸‹æ¸¸ä»»åŠ¡ç»“æœè®°å½•")

        return downstream_df

    def calculate_changes(self, mem_df, downstream_df):
        """
        è®¡ç®—ä»baseåˆ°SFTçš„å˜åŒ–é‡

        Args:
            mem_df (pd.DataFrame): è®°å¿†åŒ–ç»“æœ
            downstream_df (pd.DataFrame): ä¸‹æ¸¸ä»»åŠ¡ç»“æœ

        Returns:
            tuple: (è®°å¿†åŒ–å˜åŒ–æ•°æ®æ¡†, ä¸‹æ¸¸ä»»åŠ¡å˜åŒ–æ•°æ®æ¡†)
        """
        print("ğŸ“Š è®¡ç®—ä»baseåˆ°SFTçš„å˜åŒ–é‡...")

        # è®¡ç®—è®°å¿†åŒ–å˜åŒ–ï¼ˆchange_1ï¼‰
        mem_changes = []

        for scale in self.scales:
            for dataset in self.datasets:
                # è·å–baseå’Œsftç»“æœ
                base_data = mem_df[(mem_df['model_type'] == 'base') &
                                   (mem_df['model_scale'] == scale) &
                                   (mem_df['dataset'] == dataset)]
                sft_data = mem_df[(mem_df['model_type'] == 'sft') &
                                  (mem_df['model_scale'] == scale) &
                                  (mem_df['dataset'] == dataset)]

                if len(base_data) > 0 and len(sft_data) > 0:
                    change_record = {
                        'scale': scale,
                        'dataset': dataset
                    }

                    # è®¡ç®—å„æŒ‡æ ‡çš„å˜åŒ–é‡
                    for metric in self.mem_metrics:
                        if metric in base_data.columns and metric in sft_data.columns:
                            base_val = base_data[metric].iloc[0]
                            sft_val = sft_data[metric].iloc[0]

                            # ç»å¯¹å˜åŒ–é‡
                            change_record[f'{metric}_change_abs'] = sft_val - base_val

                            # ç›¸å¯¹å˜åŒ–é‡ï¼ˆé¿å…é™¤ä»¥0ï¼‰
                            if base_val != 0:
                                change_record[f'{metric}_change_rel'] = (sft_val - base_val) / base_val
                            else:
                                change_record[f'{metric}_change_rel'] = 0.0

                    mem_changes.append(change_record)

        mem_changes_df = pd.DataFrame(mem_changes)
        print(f"   ğŸ“ˆ è®¡ç®—äº† {len(mem_changes_df)} æ¡è®°å¿†åŒ–å˜åŒ–è®°å½•")

        # è®¡ç®—ä¸‹æ¸¸ä»»åŠ¡å˜åŒ–ï¼ˆchange_2ï¼‰
        downstream_changes = []

        for scale in self.scales:
            base_data = downstream_df[(downstream_df['model_type'] == 'base') &
                                      (downstream_df['scale'] == scale)]
            sft_data = downstream_df[(downstream_df['model_type'] == 'sft') &
                                     (downstream_df['scale'] == scale)]

            if len(base_data) > 0 and len(sft_data) > 0:
                change_record = {'scale': scale}

                # è®¡ç®—å„ä»»åŠ¡çš„å˜åŒ–é‡
                for task in self.downstream_tasks:
                    base_val = base_data[task].iloc[0]
                    sft_val = sft_data[task].iloc[0]

                    # ç»å¯¹å˜åŒ–é‡
                    change_record[f'{task}_change_abs'] = sft_val - base_val

                    # ç›¸å¯¹å˜åŒ–é‡
                    if base_val != 0:
                        change_record[f'{task}_change_rel'] = (sft_val - base_val) / base_val
                    else:
                        change_record[f'{task}_change_rel'] = 0.0

                downstream_changes.append(change_record)

        downstream_changes_df = pd.DataFrame(downstream_changes)
        print(f"   ğŸ“ˆ è®¡ç®—äº† {len(downstream_changes_df)} æ¡ä¸‹æ¸¸ä»»åŠ¡å˜åŒ–è®°å½•")

        return mem_changes_df, downstream_changes_df

    def correlation_analysis(self, mem_changes_df, downstream_changes_df):
        """
        è¿›è¡Œç›¸å…³æ€§åˆ†æ

        Args:
            mem_changes_df (pd.DataFrame): è®°å¿†åŒ–å˜åŒ–æ•°æ®
            downstream_changes_df (pd.DataFrame): ä¸‹æ¸¸ä»»åŠ¡å˜åŒ–æ•°æ®

        Returns:
            dict: ç›¸å…³æ€§åˆ†æç»“æœ
        """
        print("ğŸ“Š è¿›è¡Œç›¸å…³æ€§åˆ†æ...")

        correlation_results = {
            'pearson': {},
            'spearman': {},
            'correlation_matrix': {}
        }

        # ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ†åˆ«è¿›è¡Œåˆ†æ
        for dataset in self.datasets:
            print(f"   ğŸ” åˆ†ææ•°æ®é›†: {dataset}")

            dataset_mem = mem_changes_df[mem_changes_df['dataset'] == dataset]

            # åˆå¹¶æ•°æ®ï¼ˆæŒ‰scaleï¼‰
            merged_data = pd.merge(dataset_mem, downstream_changes_df, on='scale', how='inner')

            if len(merged_data) == 0:
                print(f"     âŒ {dataset} æ•°æ®é›†æ²¡æœ‰åŒ¹é…çš„è®°å½•")
                continue

            # é€‰æ‹©æ•°å€¼åˆ—è¿›è¡Œç›¸å…³æ€§åˆ†æ
            numeric_cols = merged_data.select_dtypes(include=[np.number]).columns

            # è®¡ç®—Pearsonå’ŒSpearmanç›¸å…³ç³»æ•°
            dataset_correlations = {}

            # è®°å¿†åŒ–æŒ‡æ ‡ä¸ä¸‹æ¸¸ä»»åŠ¡çš„ç›¸å…³æ€§
            for mem_metric in self.mem_metrics:
                mem_col_abs = f'{mem_metric}_change_abs'
                mem_col_rel = f'{mem_metric}_change_rel'

                if mem_col_abs in merged_data.columns:
                    for task in self.downstream_tasks:
                        task_col_abs = f'{task}_change_abs'
                        task_col_rel = f'{task}_change_rel'

                        if task_col_abs in merged_data.columns:
                            # Pearsonç›¸å…³æ€§
                            try:
                                pearson_corr, pearson_p = pearsonr(
                                    merged_data[mem_col_abs].dropna(),
                                    merged_data[task_col_abs].dropna()
                                )

                                spearman_corr, spearman_p = spearmanr(
                                    merged_data[mem_col_abs].dropna(),
                                    merged_data[task_col_abs].dropna()
                                )

                                key = f"{mem_metric}_vs_{task}"
                                dataset_correlations[key] = {
                                    'pearson_corr': pearson_corr,
                                    'pearson_p': pearson_p,
                                    'spearman_corr': spearman_corr,
                                    'spearman_p': spearman_p,
                                    'sample_size': len(merged_data[mem_col_abs].dropna())
                                }

                            except Exception as e:
                                print(f"     âš ï¸  è®¡ç®—{mem_metric}ä¸{task}ç›¸å…³æ€§æ—¶å‡ºé”™: {e}")

            correlation_results['pearson'][dataset] = dataset_correlations
            correlation_results['spearman'][dataset] = dataset_correlations

            # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
            if len(numeric_cols) > 1:
                corr_matrix = merged_data[numeric_cols].corr()
                correlation_results['correlation_matrix'][dataset] = corr_matrix

        return correlation_results

    def causal_analysis(self, mem_changes_df, downstream_changes_df):
        """
        è¿›è¡Œå› æœåˆ†æï¼ˆä½¿ç”¨çº¿æ€§å›å½’å’Œéšæœºæ£®æ—ï¼‰

        Args:
            mem_changes_df (pd.DataFrame): è®°å¿†åŒ–å˜åŒ–æ•°æ®
            downstream_changes_df (pd.DataFrame): ä¸‹æ¸¸ä»»åŠ¡å˜åŒ–æ•°æ®

        Returns:
            dict: å› æœåˆ†æç»“æœ
        """
        print("ğŸ“Š è¿›è¡Œå› æœåˆ†æ...")

        causal_results = {
            'linear_regression': {},
            'random_forest': {},
            'feature_importance': {}
        }

        for dataset in self.datasets:
            print(f"   ğŸ” åˆ†ææ•°æ®é›†: {dataset}")

            dataset_mem = mem_changes_df[mem_changes_df['dataset'] == dataset]
            merged_data = pd.merge(dataset_mem, downstream_changes_df, on='scale', how='inner')

            if len(merged_data) < 3:  # éœ€è¦è‡³å°‘3ä¸ªæ•°æ®ç‚¹è¿›è¡Œå›å½’
                print(f"     âŒ {dataset} æ•°æ®ç‚¹å¤ªå°‘ï¼Œè·³è¿‡å› æœåˆ†æ")
                continue

            # å‡†å¤‡ç‰¹å¾ï¼ˆè®°å¿†åŒ–å˜åŒ–ä½œä¸ºè¾“å…¥ï¼‰
            feature_cols = []
            for metric in self.mem_metrics:
                col_abs = f'{metric}_change_abs'
                col_rel = f'{metric}_change_rel'
                if col_abs in merged_data.columns:
                    feature_cols.append(col_abs)
                if col_rel in merged_data.columns:
                    feature_cols.append(col_rel)

            if len(feature_cols) == 0:
                print(f"     âŒ {dataset} æ²¡æœ‰å¯ç”¨çš„è®°å¿†åŒ–ç‰¹å¾")
                continue

            X = merged_data[feature_cols].fillna(0)

            dataset_causal = {}

            # å¯¹æ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œé¢„æµ‹
            for task in self.downstream_tasks:
                target_col = f'{task}_change_abs'

                if target_col in merged_data.columns:
                    y = merged_data[target_col].fillna(0)

                    if len(y.unique()) <= 1:  # å¦‚æœç›®æ ‡å˜é‡æ²¡æœ‰å˜åŒ–ï¼Œè·³è¿‡
                        continue

                    try:
                        # çº¿æ€§å›å½’
                        lr_model = LinearRegression()
                        lr_model.fit(X, y)
                        lr_pred = lr_model.predict(X)
                        lr_r2 = r2_score(y, lr_pred)
                        lr_mse = mean_squared_error(y, lr_pred)

                        # éšæœºæ£®æ—
                        rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
                        rf_model.fit(X, y)
                        rf_pred = rf_model.predict(X)
                        rf_r2 = r2_score(y, rf_pred)
                        rf_mse = mean_squared_error(y, rf_pred)

                        dataset_causal[task] = {
                            'linear_regression': {
                                'r2_score': lr_r2,
                                'mse': lr_mse,
                                'coefficients': lr_model.coef_.tolist(),
                                'intercept': lr_model.intercept_
                            },
                            'random_forest': {
                                'r2_score': rf_r2,
                                'mse': rf_mse,
                                'feature_importance': rf_model.feature_importances_.tolist()
                            }
                        }

                    except Exception as e:
                        print(f"     âš ï¸  {task}å› æœåˆ†æå‡ºé”™: {e}")

            causal_results['linear_regression'][dataset] = dataset_causal
            causal_results['random_forest'][dataset] = dataset_causal

            # ç‰¹å¾é‡è¦æ€§åˆ†æ
            if dataset_causal:
                importance_summary = {}
                for task, results in dataset_causal.items():
                    if 'random_forest' in results:
                        importance_summary[task] = dict(
                            zip(feature_cols, results['random_forest']['feature_importance']))

                causal_results['feature_importance'][dataset] = importance_summary

        return causal_results

    def save_results(self, mem_df, downstream_df, mem_changes_df, downstream_changes_df,
                     correlation_results, causal_results):
        """
        ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶

        Args:
            mem_df: åŸå§‹è®°å¿†åŒ–æ•°æ®
            downstream_df: åŸå§‹ä¸‹æ¸¸ä»»åŠ¡æ•°æ®
            mem_changes_df: è®°å¿†åŒ–å˜åŒ–æ•°æ®
            downstream_changes_df: ä¸‹æ¸¸ä»»åŠ¡å˜åŒ–æ•°æ®
            correlation_results: ç›¸å…³æ€§åˆ†æç»“æœ
            causal_results: å› æœåˆ†æç»“æœ
        """
        print("ğŸ’¾ ä¿å­˜åˆ†æç»“æœ...")

        # ä¿å­˜åŸå§‹æ•°æ®
        mem_df.to_csv(os.path.join(self.output_dir, f"{self.save_prefix}_memorization_raw.csv"), index=False)
        downstream_df.to_csv(os.path.join(self.output_dir, f"{self.save_prefix}_downstream_raw.csv"), index=False)

        # ä¿å­˜å˜åŒ–æ•°æ®
        mem_changes_df.to_csv(os.path.join(self.output_dir, f"{self.save_prefix}_memorization_changes.csv"),
                              index=False)
        downstream_changes_df.to_csv(os.path.join(self.output_dir, f"{self.save_prefix}_downstream_changes.csv"),
                                     index=False)

        # ä¿å­˜åˆ†æç»“æœï¼ˆJSONæ ¼å¼ï¼‰
        with open(os.path.join(self.output_dir, f"{self.save_prefix}_correlation_results.json"), 'w',
                  encoding='utf-8') as f:
            json.dump(correlation_results, f, indent=2, ensure_ascii=False, default=str)

        with open(os.path.join(self.output_dir, f"{self.save_prefix}_causal_results.json"), 'w', encoding='utf-8') as f:
            json.dump(causal_results, f, indent=2, ensure_ascii=False, default=str)

        # ä¿å­˜ç»¼åˆç»“æœæ‘˜è¦
        summary = {
            'experiment_info': {
                'scales': self.scales,
                'datasets': self.datasets,
                'memorization_metrics': self.mem_metrics,
                'downstream_tasks': self.downstream_tasks,
                'total_memorization_records': len(mem_df),
                'total_downstream_records': len(downstream_df),
                'memorization_changes_records': len(mem_changes_df),
                'downstream_changes_records': len(downstream_changes_df)
            },
            'analysis_results': {
                'correlation_analysis': correlation_results,
                'causal_analysis': causal_results
            }
        }

        with open(os.path.join(self.output_dir, f"{self.save_prefix}_summary.json"), 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False, default=str)

        print(f"   âœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")

    def print_results_summary(self, correlation_results, causal_results):
        """
        æ‰“å°åˆ†æç»“æœæ‘˜è¦

        Args:
            correlation_results: ç›¸å…³æ€§åˆ†æç»“æœ
            causal_results: å› æœåˆ†æç»“æœ
        """
        print("\n" + "=" * 80)
        print("ğŸ“Š å®éªŒäºŒåˆ†æç»“æœæ‘˜è¦")
        print("=" * 80)

        # ç›¸å…³æ€§åˆ†ææ‘˜è¦
        print("\nğŸ”— ç›¸å…³æ€§åˆ†æç»“æœ:")
        for dataset in self.datasets:
            if dataset in correlation_results.get('pearson', {}):
                print(f"\n  ğŸ“‹ æ•°æ®é›†: {dataset}")
                dataset_corr = correlation_results['pearson'][dataset]

                if dataset_corr:
                    # æ‰¾åˆ°æœ€å¼ºçš„ç›¸å…³æ€§
                    max_corr = 0
                    max_pair = ""

                    for pair, stats in dataset_corr.items():
                        if abs(stats['pearson_corr']) > abs(max_corr):
                            max_corr = stats['pearson_corr']
                            max_pair = pair

                    print(f"    ğŸ† æœ€å¼ºç›¸å…³æ€§: {max_pair}")
                    print(f"    ğŸ“ˆ Pearsonç›¸å…³ç³»æ•°: {max_corr:.4f}")

                    # ç»Ÿè®¡æ˜¾è‘—ç›¸å…³çš„æ•°é‡
                    significant_count = sum(1 for stats in dataset_corr.values()
                                            if stats['pearson_p'] < 0.05)
                    print(f"    âœ… æ˜¾è‘—ç›¸å…³å¯¹æ•° (p<0.05): {significant_count}/{len(dataset_corr)}")
                else:
                    print("    âŒ æ²¡æœ‰è®¡ç®—å‡ºç›¸å…³æ€§ç»“æœ")

        # å› æœåˆ†ææ‘˜è¦
        print("\nğŸ¯ å› æœåˆ†æç»“æœ:")
        for dataset in self.datasets:
            if dataset in causal_results.get('linear_regression', {}):
                print(f"\n  ğŸ“‹ æ•°æ®é›†: {dataset}")
                dataset_causal = causal_results['linear_regression'][dataset]

                if dataset_causal:
                    # æ‰¾åˆ°é¢„æµ‹æ•ˆæœæœ€å¥½çš„ä»»åŠ¡
                    max_r2 = 0
                    best_task = ""

                    for task, results in dataset_causal.items():
                        if 'linear_regression' in results:
                            r2 = results['linear_regression']['r2_score']
                            if r2 > max_r2:
                                max_r2 = r2
                                best_task = task

                    if best_task:
                        print(f"    ğŸ† æœ€ä½³é¢„æµ‹ä»»åŠ¡: {best_task}")
                        print(f"    ğŸ“ˆ çº¿æ€§å›å½’RÂ²: {max_r2:.4f}")

                        if best_task in causal_results['random_forest'][dataset]:
                            rf_r2 = causal_results['random_forest'][dataset][best_task]['random_forest']['r2_score']
                            print(f"    ğŸŒ² éšæœºæ£®æ—RÂ²: {rf_r2:.4f}")

                    print(f"    ğŸ“Š å¯é¢„æµ‹ä»»åŠ¡æ•°: {len(dataset_causal)}")
                else:
                    print("    âŒ æ²¡æœ‰è®¡ç®—å‡ºå› æœåˆ†æç»“æœ")

        print("\n" + "=" * 80)

    def visualize_results(self, mem_changes_df, downstream_changes_df, correlation_results):
        """
        å¯è§†åŒ–åˆ†æç»“æœ

        Args:
            mem_changes_df: è®°å¿†åŒ–å˜åŒ–æ•°æ®
            downstream_changes_df: ä¸‹æ¸¸ä»»åŠ¡å˜åŒ–æ•°æ®
            correlation_results: ç›¸å…³æ€§åˆ†æç»“æœ
        """
        print("ğŸ“Š å¼€å§‹åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")

        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('default')
        sns.set_palette("husl")

        # 1. è®°å¿†åŒ–å˜åŒ–è¶‹åŠ¿å›¾
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('å®éªŒäºŒï¼šè®°å¿†åŒ–ä¸ä¸‹æ¸¸ä»»åŠ¡å˜åŒ–åˆ†æ', fontsize=16, fontweight='bold')

        # 1.1 è®°å¿†åŒ–æŒ‡æ ‡å˜åŒ–ï¼ˆæŒ‰è§„æ¨¡ï¼‰
        ax1 = axes[0, 0]
        mem_pivot = mem_changes_df.pivot_table(
            index='scale',
            columns='dataset',
            values='exact_match_rate_change_abs',
            aggfunc='mean'
        )

        if not mem_pivot.empty:
            sns.heatmap(mem_pivot, annot=True, fmt='.4f', ax=ax1, cmap='RdBu_r')
            ax1.set_title('ç²¾ç¡®åŒ¹é…ç‡å˜åŒ–çƒ­åŠ›å›¾\n(SFT - Base)')
            ax1.set_xlabel('æ•°æ®é›†')
            ax1.set_ylabel('æ¨¡å‹è§„æ¨¡')
        else:
            ax1.text(0.5, 0.5, 'æ•°æ®ä¸è¶³', ha='center', va='center', transform=ax1.transAxes)
            ax1.set_title('ç²¾ç¡®åŒ¹é…ç‡å˜åŒ–çƒ­åŠ›å›¾')

        # 1.2 ä¸‹æ¸¸ä»»åŠ¡å˜åŒ–ï¼ˆæŒ‰è§„æ¨¡ï¼‰
        ax2 = axes[0, 1]
        if not downstream_changes_df.empty:
            # é€‰æ‹©å‡ ä¸ªä¸»è¦ä»»åŠ¡è¿›è¡Œå±•ç¤º
            main_tasks = ['IFEval_change_abs', 'MMLU_change_abs', 'GSM8K_change_abs']
            available_tasks = [task for task in main_tasks if task in downstream_changes_df.columns]

            if available_tasks:
                downstream_changes_df.set_index('scale')[available_tasks].plot(kind='bar', ax=ax2)
                ax2.set_title('ä¸»è¦ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½å˜åŒ–\n(SFT - Base)')
                ax2.set_xlabel('æ¨¡å‹è§„æ¨¡')
                ax2.set_ylabel('æ€§èƒ½å˜åŒ–')
                ax2.legend(title='ä»»åŠ¡', bbox_to_anchor=(1.05, 1), loc='upper left')
                ax2.tick_params(axis='x', rotation=0)
        else:
            ax2.text(0.5, 0.5, 'æ•°æ®ä¸è¶³', ha='center', va='center', transform=ax2.transAxes)
            ax2.set_title('ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½å˜åŒ–')

        # 1.3 ç›¸å…³æ€§çŸ©é˜µç¤ºä¾‹ï¼ˆé€‰æ‹©ä¸€ä¸ªæ•°æ®é›†ï¼‰
        ax3 = axes[1, 0]
        if correlation_results.get('correlation_matrix'):
            # é€‰æ‹©ç¬¬ä¸€ä¸ªæœ‰æ•°æ®çš„æ•°æ®é›†
            dataset_with_data = None
            for dataset in self.datasets:
                if dataset in correlation_results['correlation_matrix']:
                    dataset_with_data = dataset
                    break

            if dataset_with_data:
                corr_matrix = correlation_results['correlation_matrix'][dataset_with_data]

                # é€‰æ‹©éƒ¨åˆ†ç›¸å…³æ€§è¿›è¡Œå±•ç¤ºï¼ˆé¿å…å›¾è¡¨è¿‡äºå¤æ‚ï¼‰
                display_cols = [col for col in corr_matrix.columns if 'change_abs' in col][:10]
                if display_cols:
                    display_matrix = corr_matrix.loc[display_cols, display_cols]
                    sns.heatmap(display_matrix, annot=True, fmt='.2f', ax=ax3,
                                cmap='coolwarm', center=0, square=True)
                    ax3.set_title(f'ç›¸å…³æ€§çŸ©é˜µ ({dataset_with_data})')
                    ax3.tick_params(axis='both', rotation=45)

        if not hasattr(ax3, 'collections') or not ax3.collections:
            ax3.text(0.5, 0.5, 'æ•°æ®ä¸è¶³', ha='center', va='center', transform=ax3.transAxes)
            ax3.set_title('ç›¸å…³æ€§çŸ©é˜µ')

        # 1.4 è§„æ¨¡vså˜åŒ–è¶‹åŠ¿
        ax4 = axes[1, 1]

        # åˆ›å»ºè§„æ¨¡æ•°å€¼æ˜ å°„
        scale_mapping = {'1B': 1, '7B': 7, '13B': 13, '32B': 32}

        if not mem_changes_df.empty:
            # è®¡ç®—æ¯ä¸ªè§„æ¨¡ä¸‹è®°å¿†åŒ–çš„å¹³å‡å˜åŒ–
            mem_avg_changes = []
            scales_numeric = []

            for scale in self.scales:
                scale_data = mem_changes_df[mem_changes_df['scale'] == scale]
                if not scale_data.empty:
                    avg_change = scale_data['exact_match_rate_change_abs'].mean()
                    if not np.isnan(avg_change):
                        mem_avg_changes.append(avg_change)
                        scales_numeric.append(scale_mapping[scale])

            if mem_avg_changes:
                ax4.plot(scales_numeric, mem_avg_changes, 'o-', label='è®°å¿†åŒ–å˜åŒ–', linewidth=2, markersize=8)

        if not downstream_changes_df.empty:
            # è®¡ç®—æ¯ä¸ªè§„æ¨¡ä¸‹ä¸‹æ¸¸ä»»åŠ¡çš„å¹³å‡å˜åŒ–
            downstream_avg_changes = []
            scales_numeric_downstream = []

            # è®¡ç®—æ‰€æœ‰ä¸‹æ¸¸ä»»åŠ¡å˜åŒ–çš„å¹³å‡å€¼
            change_cols = [col for col in downstream_changes_df.columns if 'change_abs' in col]
            if change_cols:
                downstream_changes_df['avg_change'] = downstream_changes_df[change_cols].mean(axis=1)

                for scale in self.scales:
                    scale_data = downstream_changes_df[downstream_changes_df['scale'] == scale]
                    if not scale_data.empty:
                        avg_change = scale_data['avg_change'].mean()
                        if not np.isnan(avg_change):
                            downstream_avg_changes.append(avg_change)
                            scales_numeric_downstream.append(scale_mapping[scale])

                if downstream_avg_changes:
                    ax4.plot(scales_numeric_downstream, downstream_avg_changes, 's-',
                             label='ä¸‹æ¸¸ä»»åŠ¡å˜åŒ–', linewidth=2, markersize=8)

        ax4.set_xlabel('æ¨¡å‹è§„æ¨¡ (B)')
        ax4.set_ylabel('å¹³å‡å˜åŒ–é‡')
        ax4.set_title('æ¨¡å‹è§„æ¨¡ vs æ€§èƒ½å˜åŒ–è¶‹åŠ¿')
        ax4.legend()
        ax4.grid(True, alpha=0.3)

        # è®¾ç½®xè½´åˆ»åº¦
        if scales_numeric or scales_numeric_downstream:
            all_scales = list(set(scales_numeric + scales_numeric_downstream))
            ax4.set_xticks(sorted(all_scales))
            ax4.set_xticklabels([f'{s}B' for s in sorted(all_scales)])

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        viz_path = os.path.join(self.output_dir, f"{self.save_prefix}_visualization.png")
        plt.savefig(viz_path, dpi=300, bbox_inches='tight')
        print(f"   âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {viz_path}")

        # 2. åˆ›å»ºè¯¦ç»†çš„ç›¸å…³æ€§åˆ†æå›¾
        self._create_correlation_plots(correlation_results)

        plt.show()

    def _create_correlation_plots(self, correlation_results):
        """
        åˆ›å»ºè¯¦ç»†çš„ç›¸å…³æ€§åˆ†æå›¾è¡¨

        Args:
            correlation_results: ç›¸å…³æ€§åˆ†æç»“æœ
        """
        if not correlation_results.get('pearson'):
            print("   âš ï¸  æ²¡æœ‰ç›¸å…³æ€§æ•°æ®ï¼Œè·³è¿‡ç›¸å…³æ€§å›¾è¡¨åˆ›å»º")
            return

        # ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ›å»ºç›¸å…³æ€§å›¾
        for dataset in self.datasets:
            if dataset not in correlation_results['pearson']:
                continue

            dataset_corr = correlation_results['pearson'][dataset]
            if not dataset_corr:
                continue

            # æå–ç›¸å…³æ€§æ•°æ®
            pairs = []
            pearson_corrs = []
            spearman_corrs = []
            p_values = []

            for pair, stats in dataset_corr.items():
                pairs.append(pair.replace('_vs_', '\nvs\n'))
                pearson_corrs.append(stats['pearson_corr'])
                spearman_corrs.append(stats['spearman_corr'])
                p_values.append(stats['pearson_p'])

            if not pairs:
                continue

            # åˆ›å»ºç›¸å…³æ€§å¯¹æ¯”å›¾
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
            fig.suptitle(f'{dataset} æ•°æ®é›†ç›¸å…³æ€§åˆ†æ', fontsize=14, fontweight='bold')

            # Pearson vs Spearman ç›¸å…³æ€§å¯¹æ¯”
            x_pos = np.arange(len(pairs))
            width = 0.35

            bars1 = ax1.bar(x_pos - width / 2, pearson_corrs, width, label='Pearson', alpha=0.8)
            bars2 = ax1.bar(x_pos + width / 2, spearman_corrs, width, label='Spearman', alpha=0.8)

            ax1.set_xlabel('è®°å¿†åŒ–æŒ‡æ ‡ vs ä¸‹æ¸¸ä»»åŠ¡')
            ax1.set_ylabel('ç›¸å…³ç³»æ•°')
            ax1.set_title('Pearson vs Spearman ç›¸å…³æ€§')
            ax1.set_xticks(x_pos)
            ax1.set_xticklabels(pairs, rotation=45, ha='right', fontsize=8)
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar in bars1:
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width() / 2., height,
                         f'{height:.3f}', ha='center', va='bottom', fontsize=7)

            # æ˜¾è‘—æ€§åˆ†æ
            significant_indices = [i for i, p in enumerate(p_values) if p < 0.05]
            colors = ['red' if p < 0.05 else 'blue' for p in p_values]

            bars3 = ax2.bar(x_pos, [-np.log10(p) for p in p_values], color=colors, alpha=0.7)
            ax2.axhline(y=-np.log10(0.05), color='red', linestyle='--', label='p=0.05')
            ax2.set_xlabel('è®°å¿†åŒ–æŒ‡æ ‡ vs ä¸‹æ¸¸ä»»åŠ¡')
            ax2.set_ylabel('-log10(p-value)')
            ax2.set_title('ç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ')
            ax2.set_xticks(x_pos)
            ax2.set_xticklabels(pairs, rotation=45, ha='right', fontsize=8)
            ax2.legend()
            ax2.grid(True, alpha=0.3)

            plt.tight_layout()

            # ä¿å­˜ç›¸å…³æ€§å›¾è¡¨
            corr_viz_path = os.path.join(self.output_dir, f"{self.save_prefix}_correlation_{dataset}.png")
            plt.savefig(corr_viz_path, dpi=300, bbox_inches='tight')
            print(f"   âœ… {dataset}ç›¸å…³æ€§å›¾è¡¨å·²ä¿å­˜: {corr_viz_path}")

    def run_analysis(self):
        """
        è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
        """
        print("ğŸš€ å¼€å§‹å®éªŒäºŒï¼šè®°å¿†åŒ–ä¸ä¸‹æ¸¸ä»»åŠ¡å…³ç³»åˆ†æ")
        print("=" * 80)

        try:
            # 1. åŠ è½½æ•°æ®
            mem_df = self.load_memorization_results()
            downstream_df = self.create_downstream_results()

            # 2. è®¡ç®—å˜åŒ–é‡
            mem_changes_df, downstream_changes_df = self.calculate_changes(mem_df, downstream_df)

            # 3. ç›¸å…³æ€§åˆ†æ
            correlation_results = self.correlation_analysis(mem_changes_df, downstream_changes_df)

            # 4. å› æœåˆ†æ
            causal_results = self.causal_analysis(mem_changes_df, downstream_changes_df)

            # 5. ä¿å­˜ç»“æœ
            self.save_results(mem_df, downstream_df, mem_changes_df, downstream_changes_df,
                              correlation_results, causal_results)

            # 6. æ‰“å°ç»“æœæ‘˜è¦
            self.print_results_summary(correlation_results, causal_results)

            # 7. å¯è§†åŒ–
            self.visualize_results(mem_changes_df, downstream_changes_df, correlation_results)

            print("\nğŸ‰ å®éªŒäºŒåˆ†æå®Œæˆï¼")
            print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")

        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()


def main():
    """
    ä¸»å‡½æ•°ï¼šè®¾ç½®å‚æ•°å¹¶è¿è¡Œåˆ†æ
    """
    parser = argparse.ArgumentParser(description="å®éªŒäºŒï¼šè®°å¿†åŒ–ä¸ä¸‹æ¸¸ä»»åŠ¡å…³ç³»åˆ†æ")

    parser.add_argument(
        '--memorization_dir',
        type=str,
        default='/root/autodl-tmp/ift_memorization/results/exp1_mem_score',
        help='è®°å¿†åŒ–ç»“æœç›®å½•è·¯å¾„'
    )

    parser.add_argument(
        '--output_dir',
        type=str,
        default='/root/autodl-tmp/ift_memorization/results/exp2_relationship_analysis',
        help='è¾“å‡ºç›®å½•è·¯å¾„'
    )

    parser.add_argument(
        '--save_prefix',
        type=str,
        default='exp2Relation',
        help='ä¿å­˜æ–‡ä»¶çš„å‰ç¼€'
    )

    args = parser.parse_args()

    print("ğŸ“‹ åˆ†æå‚æ•°:")
    print(f"   ğŸ“‚ è®°å¿†åŒ–ç»“æœç›®å½•: {args.memorization_dir}")
    print(f"   ğŸ“‚ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"   ğŸ·ï¸  ä¿å­˜å‰ç¼€: {args.save_prefix}")
    print()



    # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œ
    analyzer = MemorizationDownstreamAnalyzer(
        memorization_dir=args.memorization_dir,
        output_dir=args.output_dir,
        save_prefix=args.save_prefix
    )

    analyzer.run_analysis()


if __name__ == "__main__":
    # ä»»åŠ¡å¯¹åº”çš„åˆ†æ•°ï¼ˆä»LaTeXè¡¨æ ¼ä¸­æå–ï¼‰ GSM8K     # MATH     # MMLU     # PopQA
    TASK_SCORES = {
        "OLMo-2-1B": [0.41, 0.06, 0.49, 0.27],
        "OLMo-2-1B-SFT": [0.38, 0.12, 0.45, 0.20],
        "OLMo-2-7B": [0.68, 0.06, 0.69, 0.35],
        "OLMo-2-7B-SFT": [0.71, 0.21, 0.67, 0.26],
        # "OLMo-2-1124-13B": [0.790, 0.000, 0.390, 0.100, 0.000, 0.950],
        # "OLMo-2-1124-13B-SFT": [],
        # "OLMo-2-0325-32B": [0.790, 0.000, 0.390, 0.100, 0.000, 0.950],46
        # "OLMo-2-0325-32B-SFT": [],
    }
    main()